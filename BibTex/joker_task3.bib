
@inproceedings{farwell_pragmatics-based_2006,
	title = {Pragmatics-based {MT} and the {Translation} of {Puns}},
	url = {http://www.mt-archive.info/EAMT-2006-Farwell.pdf},
	abstract = {This paper describes an approach to translation which which recognizes a threefold distinction among the intentions of the author of a text: the locutionary intent (how something is said), the illocutionary intent (what is being said) and the perlocutionary intent (why something is being said) (cf. Austin, 1962; Searle, 1969). We claim that all three are taken into consideration in the translation process and that any of the three may be used to justify a translator’s choices. We present a case- study of the application of the framework to the translation of puns, in particular to one pun. We show that this approach can help to identify, quantify and qualify the choices of the translators.},
	booktitle = {Proceedings of the 11th {Annual} {Conference} of the {European} {Association} for {Machine} {Translation}},
	author = {Farwell, David and Helmreich, Stephen},
	month = jun,
	year = {2006},
	pages = {187--194},
}

@inproceedings{valitutti_let_2013,
	title = {“{Let} {Everything} {Turn} {Well} in {Your} {Wife}”: {Generation} of {Adult} {Humor} {Using} {Lexical} {Constraints}},
	volume = {2},
	url = {https://aclanthology.org/P13-2044},
	booktitle = {Proceedings of the 51st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Valitutti, Alessandro and Toivonen, Hannu and Doucet, Antoine and Toivanen, Jukka M.},
	month = aug,
	year = {2013},
	pages = {243--248},
}

@inproceedings{hong_automatically_2009,
	address = {Boulder, Colorado},
	title = {Automatically {Extracting} {Word} {Relationships} as {Templates} for {Pun} {Generation}},
	url = {https://aclanthology.org/W09-2004},
	booktitle = {Proceedings of the {Workshop} on {Computational} {Approaches} to {Linguistic} {Creativity}},
	publisher = {Association for Computational Linguistics},
	author = {Hong, Bryan Anthony and Ong, Ethel},
	month = jun,
	year = {2009},
	pages = {24--31},
}

@book{noauthor_computational_2009,
	title = {Computational {Approaches} to {Linguistic} {Creativity}: {Proceedings} of the {Workshop}},
	isbn = {978-1-932432-36-7},
	publisher = {Association for Computational Linguistics},
	month = jun,
	year = {2009},
}

@inproceedings{yu_neural_2018,
	title = {A {Neural} {Approach} to {Pun} {Generation}},
	volume = {1},
	url = {https://aclanthology.org/P18-1153},
	doi = {10.18653/v1/P18-1153},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Zhiwei and Tan, Jiwei and Wan, Xiaojun},
	month = jul,
	year = {2018},
	pages = {1650--1660},
}

@inproceedings{nijholt_humor_2017,
	title = {Humor in {Human}-{Computer} {Interaction}: a {Short} {Survey}},
	isbn = {978-81-931260-9-7},
	url = {https://www.interact2017.org/downloads/INTERACT_2017_Adjunct_v4_final_24jan.pdf},
	booktitle = {Adjunct {Proceedings}: {INTERACT} 2017 {Mumbai}},
	publisher = {Industrial Design Centre, Indian Institute of Technology Bombay},
	author = {Nijholt, Anton and Niculescu, Andreea and Valitutti, Alessandro and Banchs, Rafael Enrique},
	editor = {Joshi, Anirudha and Balkrishan, Devanuj K. and Dalvi, Girish and Winckler, Marco},
	year = {2017},
	pages = {199--220},
}

@inproceedings{ghanem_idatfire2019_2019,
	title = {{IDAT}@{FIRE2019}: {Overview} of the {Track} on {Irony} {Detection} in {Arabic} {Tweets}},
	doi = {10.1145/3368567.3368585},
	booktitle = {Proceedings of the 11th {Forum} for {Information} {Retrieval} {Evaluation}},
	publisher = {Association for Computing Machinery},
	author = {Ghanem, Bilal and Karoui, Jihen and Benamara, Farah and Moriceau, Veronique and Rosso, Paolo},
	year = {2019},
	pages = {10--13},
}

@inproceedings{karoui_exploring_2017,
	title = {Exploring the {Impact} of {Pragmatic} {Phenomena} on {Irony} {Detection} in {Tweets}: a {Multilingual} {Corpus} {Study}},
	volume = {1},
	url = {https://oatao.univ-toulouse.fr/18921/},
	booktitle = {15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Karoui, Jihen and Benamara, Farah and Moriceau, Véronique and Patti, Viviana and Bosco, Cristina and Aussenac-Gilles, Nathalie},
	year = {2017},
	pages = {262--272},
}

@inproceedings{karoui_towards_2015,
	title = {Towards a {Contextual} {Pragmatic} {Model} to {Detect} {Irony} in {Tweets}},
	volume = {2},
	url = {http://aclweb.org/anthology/P15-2106},
	doi = {10.3115/v1/P15-2106},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Karoui, Jihen and Farah, Benamara and Moriceau, Véronique and Aussenac-Gilles, Nathalie and Hadrich-Belguith, Lamia},
	year = {2015},
	pages = {644--650},
}

@article{reyes_humor_2012,
	title = {From {Humor} {Recognition} to {Irony} {Detection}: the {Figurative} {Language} of {Social} {Media}},
	volume = {74},
	issn = {0169-023X},
	doi = {10.1016/j.datak.2012.02.005},
	journal = {Data \& Knowledge Engineering},
	author = {Reyes, Antonio and Rosso, Paolo and Buscaldi, Davide},
	month = apr,
	year = {2012},
	pages = {1--12},
}

@inproceedings{guibon_multilingual_2019,
	address = {La Rochelle, France},
	title = {Multilingual {Fake} {News} {Detection} with {Satire}},
	url = {https://halshs.archives-ouvertes.fr/halshs-02391141},
	booktitle = {{CICLing}: {International} {Conference} on {Computational} {Linguistics} and {Intelligent} {Text} {Processing}},
	author = {Guibon, Gaël and Ermakova, Liana and Seffih, Hosni and Firsov, Anton and Le Noé-Bienvenu, Guillaume},
	month = apr,
	year = {2019},
	keywords = {machine learning, artificial intelligence, natural language processing, fake news, deception, satire},
}

@inproceedings{francesconi_error_2018,
	title = {Error {Analysis} in a {Hate} {Speech} {Detection} {Task}: the {Case} of {HaSpeeDe}-{TW} at {EVALITA} 2018},
	url = {http://ceur-ws.org/Vol-2481/paper32.pdf},
	abstract = {Taking as a case study the Hate Speech Detection task at EVALITA 2018, the paper discusses the distribution and typology of the errors made by the ﬁve bestscoring systems. The focus is on the subtask where Twitter data was used both for training and testing (HaSpeeDe-TW). In order to highlight the complexity of hate speech and the reasons beyond the failures in its automatic detection, the annotation provided for the task is enriched with orthogonal categories annotated in the original reference corpus, such as aggressiveness, offensiveness, irony and the presence of stereotypes.},
	booktitle = {Proceedings of the 6th {Italian} {Conference} on {Computational} {Linguistics}},
	author = {Francesconi, Chiara and Bosco, Cristina and Poletto, Fabio and Sanguinetti, Manuela},
	editor = {Bernardi, Raffaella and Navigli, Roberto and Semeraro, Giovanni},
	month = nov,
	year = {2018},
}

@inproceedings{potash_semeval-2017_2017,
	title = {{SemEval}-2017 {Task} 6: \#{HashtagWars}: {Learning} a {Sense} of {Humor}},
	isbn = {978-1-945626-55-5},
	doi = {10.18653/v1/S17-2004},
	booktitle = {Proceedings of the 11th {International} {Workshop} on {Semantic} {Evaluation}},
	publisher = {Association for Computational Linguistics},
	author = {Potash, Peter and Romanov, Alexey and Rumshisky, Anna},
	month = aug,
	year = {2017},
	pages = {49--57},
}

@inproceedings{hossain_semeval-2020_2020,
	address = {Barcelona (online)},
	title = {{SemEval}-2020 {Task} 7: {Assessing} {Humor} in {Edited} {News} {Headlines}},
	url = {https://aclanthology.org/2020.semeval-1.98},
	doi = {10.18653/v1/2020.semeval-1.98},
	abstract = {This paper describes the SemEval-2020 shared task “Assessing Humor in Edited News Headlines.” The task's dataset contains news headlines in which short edits were applied to make them funny, and the funniness of these edited headlines was rated using crowdsourcing. This task includes two subtasks, the first of which is to estimate the funniness of headlines on a humor scale in the interval 0-3. The second subtask is to predict, for a pair of edited versions of the same original headline, which is the funnier version. To date, this task is the most popular shared computational humor task, attracting 48 teams for the first subtask and 31 teams for the second.},
	booktitle = {Proceedings of the {Fourteenth} {Workshop} on {Semantic} {Evaluation}},
	publisher = {International Committee for Computational Linguistics},
	author = {Hossain, Nabil and Krumm, John and Gamon, Michael and Kautz, Henry},
	month = dec,
	year = {2020},
	pages = {746--758},
}

@inproceedings{meaney_semeval-2021_2021,
	title = {{SemEval}-2021 {Task} 7: {HaHackathon}, {Detecting} and {Rating} {Humor} and {Offense}},
	url = {https://aclanthology.org/2021.semeval-1.9},
	doi = {10.18653/v1/2021.semeval-1.9},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation}},
	publisher = {Association for Computational Linguistics},
	author = {Meaney, J. A. and Wilson, Steven and Chiruzzo, Luis and Lopez, Adam and Magdy, Walid},
	month = aug,
	year = {2021},
	pages = {105--119},
}

@inproceedings{mihalcea_making_2005,
	address = {Stroudsburg, PA},
	title = {Making {Computers} {Laugh}: {Investigations} in {Automatic} {Humor} {Recognition}},
	url = {http://www.aclweb.org/anthology/H/H05/H05-1067},
	doi = {10.3115/1220575.1220642},
	booktitle = {Human {Language} {Technology} {Conference} and {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Proceedings} of the {Conference}},
	publisher = {Association for Computational Linguistics},
	author = {Mihalcea, Rada and Strapparava, Carlo},
	month = oct,
	year = {2005},
	pages = {531--538},
}

@inproceedings{cattle_recognizing_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Recognizing {Humour} {Using} {Word} {Associations} and {Humour} {Anchor} {Extraction}},
	url = {https://www.aclweb.org/anthology/C18-1157},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Cattle, Andrew and Ma, Xiaojuan},
	month = aug,
	year = {2018},
	note = {Journal Abbreviation: Proceedings of the 27th International Conference on Computational Linguistics},
	pages = {1849--1858},
}

@inproceedings{yang_humor_2015,
	title = {Humor {Recognition} and {Humor} {Anchor} {Extraction}},
	url = {https://www.aclweb.org/anthology/D15-1284},
	doi = {10.18653/v1/D15-1284},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Diyi and Lavie, Alon and Dyer, Chris and Hovy, Eduard},
	month = sep,
	year = {2015},
	pages = {2367--2376},
}

@incollection{reyes_analysis_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Analysis} of the {Impact} of {Ambiguity} on {Automatic} {Humour} {Recognition}},
	isbn = {978-3-642-04208-9},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer},
	author = {Reyes, Antonio and Buscaldi, Davide and Rosso, Paolo},
	editor = {Matoušek, Václav and Mautner, Pavel},
	year = {2009},
	doi = {10.1007/978-3-642-04208-9_25},
	pages = {162--169},
}

@inproceedings{blinov_large_2019,
	title = {Large {Dataset} and {Language} {Model} {Fun}-{Tuning} for {Humor} {Recognition}},
	doi = {10.18653/v1/P19-1394},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Blinov, Vladislav and Bolotova-Baranova, Valeria and Braslavski, Pavel},
	year = {2019},
	pages = {4027--4032},
}

@inproceedings{ermilov_stierlitz_2018,
	address = {Cham, Switzerland},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Stierlitz {Meets} {SVM}: {Humor} {Detection} in {Russian}},
	volume = {930},
	isbn = {978-3-030-01204-5},
	doi = {10.1007/978-3-030-01204-5_17},
	booktitle = {Artificial {Intelligence} and {Natural} {Language}: 7th {International} {Conference}, {AINL} 2018},
	publisher = {Springer},
	author = {Ermilov, Anton and Murashkina, Natasha and Goryacheva, Valeria and Braslavski, Pavel},
	editor = {Ustalov, Dmitry and Filchenkov, Andrey and Pivovarova, Lidia and Žižka, Jan},
	year = {2018},
	pages = {178--184},
}

@inproceedings{castro_crowd-annotated_2018,
	title = {A {Crowd}-{Annotated} {Spanish} {Corpus} for {Humor} {Analysis}},
	url = {https://www.aclweb.org/anthology/W18-3502},
	doi = {10.18653/v1/W18-3502},
	booktitle = {Proceedings of the {Sixth} {International} {Workshop} on {Natural} {Language} {Processing} for {Social} {Media}},
	publisher = {Association for Computational Linguistics},
	author = {Castro, Santiago and Chiruzzo, Luis and Rosá, Aiala and Garat, Diego and Moncecchi, Guillermo},
	month = jul,
	year = {2018},
	pages = {7--11},
}

@misc{noauthor_les_nodate,
	title = {Les {Jeux} {De} {Mots} {Dans} {Astérix} !},
	url = {https://asterixofficiel.tumblr.com/post/61658868419/les-jeux-de-mots-dans-asterix},
	note = {Publication Title: Astérix \& Obélix (Officiel)},
}

@misc{noauthor_liste_2021,
	title = {Liste {De} {Personnages} {D}'{\textbackslash}{emphAstérix}},
	url = {https://fr.wikipedia.org/w/index.php?title=Liste_de_personnages_d%27Ast%C3%A9rix&oldid=181797864},
	language = {fr},
	month = apr,
	year = {2021},
	note = {Publication Title: Wikipédia},
}

@misc{asterisk_harry_2018,
	title = {Harry {Potter} {Puns}},
	url = {https://punpedia.org/harry-potter-puns/},
	language = {en-US},
	author = {asterisk, Author},
	month = jul,
	year = {2018},
	note = {Publication Title: Punpedia},
}

@techreport{lieber_jurassic-1_2021,
	type = {White paper},
	title = {Jurassic-1: {Technical} {Details} and {Evaluation}},
	url = {https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf},
	institution = {AI21 Labs},
	author = {Lieber, Opher and Sharir, Or and Lentz, Barak and Shoham, Yoav},
	month = aug,
	year = {2021},
}

@inproceedings{xue_mt5_2021,
	title = {{mT5}: {A} {Massively} {Multilingual} {Pre}-trained {Text}-to-{Text} {Transformer}},
	url = {https://aclanthology.org/2021.naacl-main.41},
	doi = {10.18653/v1/2021.naacl-main.41},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
	month = jun,
	year = {2021},
	pages = {483--498},
}

@inproceedings{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	volume = {1},
	url = {https://doi.org/10.18653/v1/n19-1423},
	doi = {10.18653/v1/n19-1423},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	pages = {4171--4186},
}

@misc{brown_language_2020,
	title = {Language {Models} {Are} {Few}-{Shot} {Learners}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	note = {\_eprint: 2005.14165},
}

@techreport{radford_language_2019,
	type = {Technical report},
	title = {Language {Models} {Are} {Unsupervised} {Multitask} {Learners}},
	url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
	institution = {OpenAI},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
}

@article{jiang_neural_2020,
	title = {Neural {CRF} {Model} for {Sentence} {Alignment} in {Text} {Simplification}},
	url = {http://arxiv.org/abs/2005.02324},
	journal = {arXiv:2005.02324 [cs]},
	author = {Jiang, Chao and Maddela, Mounica and Lan, Wuwei and Zhong, Yang and Xu, Wei},
	month = jun,
	year = {2020},
}

@book{delabastita_theres_1993,
	address = {Amsterdam},
	title = {There's a {Double} {Tongue}: an {Investigation} into the {Translation} of {Shakespeare}'s {Wordplay}, with {Special} {Reference} to {Hamlet}},
	isbn = {978-90-5183-495-6},
	publisher = {Rodopi},
	author = {Delabastita, Dirk},
	year = {1993},
}

@article{vrticka_neural_2013,
	title = {The {Neural} {Basis} of {Humour} {Processing}},
	volume = {14},
	issn = {1471-0048},
	doi = {10.1038/nrn3566},
	number = {12},
	journal = {Nature Reviews Neuroscience},
	author = {Vrticka, Pascal and Black, Jessica M. and Reiss, Allan L.},
	month = dec,
	year = {2013},
	pages = {860--868},
}

@article{delabastita_introduction_1996,
	title = {Introduction to the {Special} {Issue} on {Wordplay} and {Translation}},
	volume = {2},
	doi = {10.1080/13556509.1996.10798970},
	number = {2},
	author = {Delabastita, Dirk},
	year = {1996},
	pages = {1--22},
}

@incollection{gottlieb_you_nodate,
	title = {You {Got} the {Picture}? {On} the {Polysemiotics} of {Subtitling} {Wordplay}},
	author = {Gottlieb, Hendrik},
	pages = {207--232},
}

@book{delabastita_traductio_1997,
	address = {Manchester},
	title = {Traductio: {Essays} on {Punning} and {Translation}},
	isbn = {978-1-900650-06-9},
	publisher = {St. Jerome},
	editor = {Delabastita, Dirk},
	year = {1997},
}

@article{giorgadze_linguistic_2014,
	title = {Linguistic {Features} of {Pun}, {Its} {Typology} and {Classification}},
	journal = {European Scientific Journal},
	author = {Giorgadze, Meri},
	year = {2014},
}

@incollection{delabastita_wordplay_2008,
	title = {Wordplay as a {Translation} {Problem}: a {Linguistic} {Perspective}},
	volume = {1},
	isbn = {978-3-11-019408-1},
	booktitle = {Ein internationales {Handbuch} zur Übersetzungsforschung},
	publisher = {De Gruyter Mouton},
	author = {Delabastita, Dirk},
	month = jul,
	year = {2008},
	doi = {10.1515/9783110137088.1.6.600},
	pages = {600--606},
}

@article{klakow_testing_2002,
	title = {Testing the {Correlation} of {Word} {Error} {Rate} and {Perplexity}},
	volume = {38},
	issn = {0167-6393},
	doi = {https://doi.org/10.1016/S0167-6393(01)00041-3},
	number = {1},
	journal = {Speech Communication},
	author = {Klakow, Dietrich and Peters, Jochen},
	year = {2002},
	pages = {19--28},
}

@inproceedings{papineni_bleu_2002,
	title = {{BLEU}: {A} {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/P02-1040},
	doi = {10.3115/1073083.1073135},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2002},
	pages = {311--318},
}

@book{hofstadter_go_2000,
	series = {Harvester studies in cognitive science},
	title = {Gödel, {Escher}, {Bach}: an {Eternal} {Golden} {Braid}},
	isbn = {978-0-14-028920-6},
	url = {https://books.google.fr/books?id=grzEQgAACAAJ},
	publisher = {Penguin},
	author = {Hofstadter, D.R.},
	year = {2000},
}

@book{henry_go_2000,
	title = {Gödel, {Escher}, {Bach}: {Les} {Brins} {D}'une {Guirlande} éternelle},
	isbn = {978-2-10-082933-0},
	url = {https://books.google.fr/books?id=W3-bzgEACAAJ},
	publisher = {Dunod.},
	author = {Henry, J. and Hofstadter, D.R. and French, R.},
	year = {2000},
}

@book{henry_traduction_2003,
	address = {Paris},
	title = {La {Traduction} {Des} {Jeux} {De} {Mots}},
	language = {fr},
	publisher = {Presses de la Sorbonne Nouvelle},
	author = {Henry, Jacqueline},
	year = {2003},
}

@article{bojanowski_enriching_2017,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	volume = {5},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00051},
	doi = {10.1162/tacl_a_00051},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	year = {2017},
	pages = {135--146},
}

@inproceedings{sennrich_neural_2016,
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	volume = {1},
	url = {http://aclweb.org/anthology/P16-1162},
	doi = {10.18653/v1/P16-1162},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	year = {2016},
	pages = {1715--1725},
}

@phdthesis{reist_thing_2018,
	type = {{PhD} {Thesis}},
	title = {A {Thing} of {Humour}? {Exploring} {Gender} and {Hostile} {Humour} in {Alberta} {Political} {Cartoons}},
	url = {https://era.library.ualberta.ca/items/b4844b19-a5a8-4380-b719-f6120121c73c},
	author = {Reist, Wilissa M},
	year = {2018},
}

@article{ford_sexist_2013,
	title = {Sexist {Humor} and {Beliefs} {That} {Justify} {Societal} {Sexism}},
	volume = {21},
	number = {7},
	journal = {Current Research in Social Psychology},
	author = {Ford, Thomas E. and Woodzicka, Julie A. and Triplett, Shane R. and Kochersberger, Annie O.},
	year = {2013},
	pages = {64--81},
}

@book{oaks_structural_2010,
	title = {Structural {Ambiguity} in {English}: an {Applied} {Grammatical} {Inventory}},
	isbn = {978-1-84706-415-8},
	abstract = {Structural Ambiguity in English is a major new scholarly work that provides an innovative and accessible linguistic description of those features of the language that can be exploited to generate structural ambiguities. Most ambiguity scholarship is concerned with disambiguation-the process of making what is ambiguous clear. This book takes the opposite approach as it focuses on describing the features in the English language that may contribute towards the creation of structural ambiguities, which form the core of some of the best word-plays found in advertising, comedy and marketing. Oaks utilizes a systematic and comprehensive inventory approach that identifies individual elements in the language and their distinctive behaviors that can be manipulated in the deliberate creation of structural ambiguities. In doing so he also provides authentic examples to illustrate the concepts he presents. This book will appeal to researchers and academics interested in the structure of the English language, usage, pragmatics, communication, natural language processing, editing, and humor studies as well as those in marketing, advertising, or humor writing.},
	publisher = {Bloomsbury},
	author = {Oaks, Dallin D.},
	year = {2010},
}

@incollection{epstein_what_2012,
	title = {What {Nonsense}: {Translating} {Neologisms}},
	isbn = {978-3-0343-0796-3},
	booktitle = {Translating {Expressive} {Language} in {Children}'s {Literature}},
	publisher = {Peter Lang},
	author = {Epstein, B. J.},
	year = {2012},
	doi = {10.3726/978-3-0353-0271-4},
	pages = {29--66},
}

@incollection{rabrenovic_you_2020,
	address = {Belgrade},
	series = {Belgrade {English} {Language} and {Literature} {Studies}},
	title = {“{You} {Just} {Fredo}-kiss {Me}?” {The} ({Non}-){Lexicalizability} of {Nonce} {Word}-{Formation}},
	volume = {1},
	isbn = {978-86-6153-616-8},
	url = {http://doi.fil.bg.ac.rs/pdf/eb_ser/bells90/2020-1/bells90-2020-1-ch9.pdf},
	abstract = {This paper analyses 19 nonce-formations taken from sitcoms, drama series and Netflix movies that refer to elements of popular culture, i.e. movies, reality shows, music, books and fairy tales. We discuss the role of both extra-linguistic knowledge and context in interpreting their meaning, and the function of these nonce-formations in discourse. The aim of this analysis is to consider whether it is at all possible for these coinages to become lexically listed. Since these nonce-formations are created in a specialized domain (the media) and communicated to a larger speech community, we conclude that in the case of nonce word-formation there is a degree of lexicalizability.},
	booktitle = {{BELLS90} {Proceedings}: {International} {Conference} to {Mark} the 90th {Anniversary} of the {English} {Department}, {Faculty} of {Philology}, {University} of {Belgrade}},
	publisher = {Faculty of Philology, University of Belgrade},
	author = {Rabrenović, Tijana},
	editor = {Čubrović, Biljana},
	year = {2020},
	doi = {10.18485/bells90.2020.1.ch9},
	pages = {153--164},
}

@incollection{braun_approaching_nodate,
	title = {Approaching {Wordplay} from the {Angle} of {Phonology} and {Phonetics} – {Examples} from {German}},
	abstract = {The present contribution seeks to outline what a phonetic approach can contribute to the study of wordplay. Therefore, it is confined to the analysis wordplay at the syllable level of language. To this end, a taxonomy of wordplay based on structural elements of the syllable is proposed. It emphasizes the distinction between wordplay relying on existing lexical items as opposed to creating new ones. Various mechanisms of “classical” wordplay are examined with respect to their effect on syllable structure. A quantitative study involving 213 items intended for a German audience is presented. Specifically, the following questions are addressed: (1) what is the distribution among the various types of wordplay at the syllable level; (2) which part of the syllable is played on, and (3) which mechanisms are most frequently used in this type of wordplay. Results show that paronymy and blending are the most frequent types of wordplay. Furthermore, there is a clear preference for the syllable onset to be played on.},
	author = {Braun, Angelika},
	doi = {10.1515/9783110501933-175},
	pages = {173--202},
}

@incollection{kolb_translation_2014,
	series = {Benjamins {Translation} {Library}},
	title = {Translation as a {Source} of {Humor}: {Jonathan} {Safran} {Foer}'s {\textbackslash}{emphEverything} is {Illuminated}{\textbackslash}{slashAlles} ist erleuchtet},
	number = {110},
	booktitle = {Transfiction: {Research} into the {Realities} of {Translation} {Fiction}},
	publisher = {John Benjamins},
	author = {Kolb, Waltraud},
	editor = {Kaindl, Klaus and Spitzl, Karlheinz},
	year = {2014},
	doi = {10.1075/btl.110.21kol},
	pages = {299--314},
}

@article{prinzl_death_2016,
	title = {Death to {Neologisms}: {Domestication} in the {English} {Retranslations} of {Thomas} {Mann}'s {\textbackslash}{emphDer} {Tod} in {Venedig}},
	volume = {5},
	doi = {10.15462/ijll.v5i3.73},
	abstract = {Thomas Mann’s Der Tod in Venedig (1912) owes much of its fame in English to a translation from 1928 by Helen Tracy Lowe-Porter. The novella however has in fact been translated many times – first by Burke (1924, with a revised edition following in 1970), and, after Lowe-Porter, by Luke (1988), Koelb (1994), Appelbaum (1995), Neugroschel (1998), Chase (1999), Heim (2004), Doege (2007) and Hansen \& Hansen (2012). Most of these versions are neither known to readers nor discussed in academic literature. This paper, which comes as part of a larger study on linguistic creativity in Der Tod in Venedig, focuses on the use of neologisms by Mann and what happens to them in (re)translation. Relying on a digital corpus composed of the complete set of English retranslations and a corpus-based methodology, the paper argues that, despite the extended time period between the publications and different translation conditions, neologisms are treated uniformly by the translators. Mann’s coinages are nearly always obliterated through normalisation and, if preserved, demonstrate less creativity overall than in the ST, raising questions about the Retranslation Hypothesis (RH) which proposes that early TT versions tend to domesticate while later ones increasingly foreignise.},
	number = {3},
	journal = {International Journal of Literary Linguistics},
	author = {Prinzl, Marlies Gabriele},
	year = {2016},
}

@inproceedings{ryskina_where_2020,
	title = {Where {New} {Words} {Are} {Born}: {Distributional} {Semantic} {Analysis} of {Neologisms} and {Their} {Semantic} {Neighborhoods}},
	url = {https://www.aclweb.org/anthology/2020.scil-1.43},
	abstract = {We perform statistical analysis of the phenomenon of neology, the process by which new words emerge in a language, using large diachronic corpora of English. We investigate the importance of two factors, semantic sparsity and frequency growth rates of semantic neighbors, formalized in the distributional semantics paradigm. We show that both factors are predictive of word emergence although we find more support for the latter hypothesis. Besides presenting a new linguistic application of distributional semantics, this study tackles the linguistic question of the role of language-internal factors (in our case, sparsity) in language change motivated by language-external factors (reflected in frequency growth).},
	booktitle = {Proceedings of the {Society} for {Computation} in {Linguistics} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Ryskina, Maria and Rabinovich, Ella and Berg-Kirkpatrick, Taylor and Mortensen, David and Tsvetkov, Yulia},
	month = jan,
	year = {2020},
	pages = {313--322},
}

@inproceedings{meyer_semi-automatic_2016,
	title = {Semi-automatic {Detection} of {Cross}-lingual {Marketing} {Blunders} {Based} on {Pragmatic} {Label} {Propagation} in {Wiktionary}},
	isbn = {978-4-87974-702-0},
	url = {https://www.aclweb.org/anthology/C16-1195},
	abstract = {We introduce the task of detecting cross-lingual marketing blunders, which occur if a trade name resembles an inappropriate or negatively connotated word in a target language. To this end, we suggest a formal task definition and a semi-automatic method based the propagation of pragmatic labels from Wiktionary across sense-disambiguated translations. Our final tool assists users by providing clues for problematic names in any language, which we simulate in two experiments on detecting previously occurred marketing blunders and identifying relevant clues for established international brands. We conclude the paper with a suggested research roadmap for this new task. To initiate further research, we publish our online demo along with the source code and data at http://uby.ukp.informatik.tu-darmstadt.de/blunder/.},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Computational} {Linguistics}},
	author = {Meyer, Christian M. and Eckle-Kohler, Judith and Gurevych, Iryna},
	month = dec,
	year = {2016},
	pages = {2071--2081},
}

@incollection{dal_playful_nodate,
	title = {Playful {Nonce}-{Formations} in {French}: {Creativity} and {Productivity}},
	abstract = {Nonce-formations, conceived as “[n]ew complex word[s] created by a speaker / writer on the spur of the moment to cover some immediate need” (Bauer 1983: 45), have been a theme in Anglo-Saxon and Germanic studies for several decades now (cf. among others Lipka 1975; Bauer 1983; Hohenhaus 1996; Crystal 2000; Štekauer 2002; Kerremans 2015), but they have received very little investigation in the French domain. Although nowadays all the conditions are met for the capture of observable data with the use of large corpora, French morphologists tend to be suspicious of individual coinages, especially if they are playful and diverge from what they consider established word formation rules. In French studies, despite the emergence of corpus-based studies, context is rarely taken into consideration, and the generative distinction between competence and performance often remains active: nonce-formations are in the scope of performance, (socio-)pragmatics or stylistics; therefore, they are not to be taken into account in morphological studies. However, nonce-formations address some interesting morphological issues: do they have to be taken into account for productivity measures? What about the clear-cut distinction between productivity and creativity? In the vein of Dal and Namer (2016a), this paper focuses on patterns of emergence of playful nonce-formations in French. After a brief definition of nonce-formations (§ 1), we first identify several recurring patterns of emergence of nonce-formations (§ 2). We then use these patterns to build a continuum among playful nonce-formations (§ 3.1). Lastly, issues related to productivity are discussed (§ 3.2).},
	author = {Dal, Georgette and Namer, Fiammetta},
	doi = {10.1515/9783110501933-205},
	pages = {203--228},
}

@incollection{winter-froemel_ludicity_nodate,
	title = {Ludicity in {Lexical} {Innovation} ({I}) – {French}},
	abstract = {The aim of this paper is to explore the importance of the ludic dimension for linguistic innovations by combining synchronic and diachronic analyses of lexicographic sources from French and by reinterpreting the data from a usage-based perspective. I will discuss the possibilities and methodological challenges in tracing ludicity in the lexicon, taking into account contemporary and historical dictionaries, most importantly Le Petit Robert 2016 and different editions of the Dictionnaire de l’Académie française. Moreover, I will analyse how innovations are introduced and perceived by speakers, distinguishing different subtypes of innovation based on structural, semantic, and pragmatic features. Finally, I will turn to the diachronic evolution of ludic innovations in order to identify general tendencies and pathways of evolution and argue that markedness plays a key role for ludic innovation, which represents an important, albeit so far neglected, domain of lexical dynamics.},
	author = {Winter-Froemel, Esme},
	doi = {10.1515/9783110501933-231},
	pages = {229--260},
}

@book{winter-froemel_dynamics_2018,
	series = {Expanding the {Lexicon}},
	title = {The {Dynamics} of {Wordplay}},
	isbn = {978-3-11-050084-4},
	number = {5},
	publisher = {De Gruyter},
	editor = {Winter-Froemel, Esme},
	year = {2018},
}

@book{raskin_semantic_1985,
	series = {Studies in {Linguistics} and {Philosophy}},
	title = {Semantic {Mechanisms} of {Humor}},
	isbn = {978-90-277-1821-1},
	number = {24},
	publisher = {Springer},
	author = {Raskin, Victor},
	year = {1985},
	doi = {10.1007/978-94-009-6472-3},
	note = {ISSN: 0924-4662},
	keywords = {linguistics, SSTH},
}

@article{attardo_script_1991,
	title = {Script {Theory} {Revis}(it)ed: {Joke} {Similarity} and {Joke} {Representation} {Model}},
	volume = {4},
	doi = {10.1515/humr.1991.4.3-4.293},
	abstract = {The article proposes a general theory of verbal humor, focusing on verbal jokes as its most representative subset. The theory is an extension and revision ofRaskin's script-based semantic theory of humor and of Attardo's five-level joke representation model. After distinguishing the parameters of the various degrees of similarity among the joke examples, six knowledge resources informing thejoke, namely script oppositions, logical mechanisms, situationst targets, narrative strategies, and language, are put forward. A hierarchical organization for the six knowledge resources is then discovered on the basis of the asymmetrical binary relations, of the proposed and modified contentl tooldichotomy, and, especially, ofthe hypothesized perceptions of the relative degrees of similarity. It is also argued that the emerging joke representation model is neutral to the process of joke production. The proposed hierarchy enables the concepts of joke variants and invariants, introduced previously by Attardo, to be firmed up, generalized, and augmented into a full-fledged taxonomy indexed with regard to the shared knowledge resource values (for example, two jokes may be variants on, that ist sharing, the same script oppositions and logical mechanisms). The resulting general theory of verbal humor is discussed in the light of its relations with various academic disciplines and areas of research as well as with the script-based semantic theory of humor, special theories of humor, and incongruity-based theories.},
	number = {3–4},
	author = {Attardo, Salvatore and Raskin, Victor},
	month = jan,
	year = {1991},
	keywords = {linguistics, GVTH},
	pages = {293--348},
}

@article{cook_automatically_2010,
	title = {Automatically {Identifying} the {Source} {Words} of {Lexical} {Blends} in {English}},
	volume = {36},
	url = {https://www.aclweb.org/anthology/J10-1005},
	doi = {10.1162/coli.2010.36.1.36104},
	abstract = {Newly coined words pose problems for natural language processing systems because they are not in a system’s lexicon, and therefore no lexical information is available for such words. A common way to form new words is lexical blending, as in {\textbackslash}emphcosmeceutical, a blend of {\textbackslash}emphcosmetic and {\textbackslash}emphpharmaceutical. We propose a statistical model for inferring a blend’s source words drawing on observed linguistic properties of blends; these properties are largely based on the recognizability of the source words in a blend. We annotate a set of 1,186 recently coined expressions which includes 515 blends, and evaluate our methods on a 324-item subset. In this first study of novel blends we achieve an accuracy of 40\% on the task of inferring a blend’s source words, which corresponds to a reduction in error rate of 39\% over an informed baseline. We also give preliminary results showing that our features for source word identification can be used to distinguish blends from other kinds of novel words.},
	number = {1},
	journal = {Computational Linguistics},
	author = {Cook, Paul and Stevenson, Suzanne},
	year = {2010},
	pages = {129--149},
}

@book{stekauer_handbook_2005,
	address = {Dordrecht, the Netherlands},
	series = {Studies in {Natural} {Language} and {Linguistic} {Theory}},
	title = {Handbook of {Word}-formation},
	isbn = {978-1-4020-3596-8},
	number = {64},
	publisher = {Springer},
	editor = {Štekauer, Pavol and Lieber, Rochelle},
	year = {2005},
	doi = {10.1007/1-4020-3596-9},
	note = {ISSN: 0924-4670},
}

@book{mattiello_analogy_2017,
	address = {Berlin, Boston},
	title = {Analogy in {Word}-formation: {A} {Study} of {English} {Neologisms} and {Occasionalisms}},
	isbn = {978-3-11-055141-9},
	url = {https://www.degruyter.com/view/title/529914},
	abstract = {This book fills a gap in lexical morphology, especially with reference to analogy in English word-formation. Many studies have focused their interest on the role played by analogy within English inflectional morphology. However, the analogical mechanism also deserves investigation on account of its relevance to neology in English. This volume provides in-depth qualitative analyses and stimulating quantitative findings in this realm.},
	publisher = {De Gruyter Mouton},
	author = {Mattiello, Elisa},
	year = {2017},
	doi = {10.1515/9783110551419},
}

@inproceedings{cartoni_lexical_2009,
	title = {Lexical {Morphology} in {Machine} {Translation}: a {Feasibility} {Study}},
	isbn = {978-1-63266-013-8},
	url = {https://bit.ly/2GyRCXa},
	abstract = {This paper presents a feasibility study for implementing lexical morphology principles in a machine translation system in order to solve unknown words. Multilingual symbolic treatment of word-formation is seducing but requires an in-depth analysis of every step that has to be performed. The construction of a prototype is firstly presented, highlighting the methodological issues of such approach. Secondly, an evaluation is performed on a large set of data, showing the benefits and the limits of such approach.},
	booktitle = {Proceedings of the 12th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Cartoni, Bruno},
	month = mar,
	year = {2009},
	pages = {130--138},
}

@inproceedings{cartoni_lexical_2008,
	title = {Lexical {Resources} for {Automatic} {Translation} of {Constructed} {Neologisms}: the {Case} {Study} of {Relational} {Adjectives}},
	isbn = {2-9517408-4-0},
	url = {https://bit.ly/2GAeMwh},
	abstract = {This paper deals with the treatment of constructed neologisms in a machine translation system. It focuses on a particular issue in Romance languages: relational adjectives and the role they play in prefixation. Relational adjectives are formally adjectives but are semantically linked to their base-noun. In prefixation processes, the prefix is formally attached to the adjective, but its semantic value(s) is applied to the semantic features of the base-noun. This phenomenon has to be taken into account by any morphological analyser or generator. Moreover, in a contrastive perspective, the possibilities of creating adjectives out of nouns are not the same in every language. We present the special mechanism we put in place to deal with this type of prefixation, and the automatic method we used to extend lexicons, so that they can retrieve the base-nouns of prefixed relational adjectives, and improve the translation quality.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Language} {Resources} and {Evaluation}},
	publisher = {European Language Resources Association (ELRA)},
	author = {Cartoni, Bruno},
	month = may,
	year = {2008},
}

@inproceedings{beinborn_cognate_2013,
	title = {Cognate {Production} {Using} {Character}-based {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/I13-1112},
	abstract = {Cognates are words in different languages that are associated with each other by language learners. Thus, cognates are important indicators for the prediction of the perceived difficulty of a text. We introduce a method for automatic cognate production using character-based machine translation. We show that our approach is able to learn production patterns from noisy training data and that it works for a wide range of language pairs. It even works across different alphabets, e.g. we obtain good results on the tested language pairs English-Russian, English-Greek, and English-Farsi. Our method performs significantly better than similarity measures used in previous work on cognates.},
	booktitle = {Proceedings of the 6th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Asian Federation of Natural Language Processing},
	author = {Beinborn, Lisa and Zesch, Torsten and Gurevych, Iryna},
	month = oct,
	year = {2013},
	pages = {883--891},
}

@article{westbury_telling_2016,
	title = {Telling the {World}'s {Least} {Funny} {Jokes}: on the {Quantification} of {Humor} as {Entropy}},
	volume = {86},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X15001023},
	doi = {10.1016/j.jml.2015.09.001},
	abstract = {In assessing aphasics or conducting experiments using a lexical decision task, we have observed informally that some non-words (NWs) reliably make people laugh. In this paper, we describe a set of studies aimed at illuminating what underlies this effect, performing the first quantitative test of a 200year old theory of humor proposed by Schopenhauer (1818). We begin with a brief overview of the history of humor theories. Schopenhauer’s theory is formulated in terms of detection/violation of patterns of co-occurrence and thereby suggests a method to quantify NW humor using Shannon entropy. A survey study demonstrates that there is much more consistency than could be expected by chance in human judgments of which NWs are funny. Analysis of that survey data and two experiments all demonstrate that Shannon entropy does indeed correctly predict human judgments of NW funniness, demonstrating as well that the perceived humor is a quantifiable function of how far the NWs are from being words.},
	journal = {Journal of Memory and Language},
	author = {Westbury, Chris and Shaoul, Cyrus and Moroschan, Gail and Ramscar, Michael},
	year = {2016},
	keywords = {Information theory, Humor, Lexical access, Non-words, Schopenhauer, Shannon},
	pages = {141--156},
}

@inproceedings{ozbal_computational_2012,
	title = {A {Computational} {Approach} to the {Automation} of {Creative} {Naming}},
	volume = {1},
	url = {https://www.aclweb.org/anthology/P12-1074},
	abstract = {In this paper, we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined. We describe all the linguistic resources and natural language processing techniques that we have exploited for this task. Then, we analyze the performance of the system that we have developed. The empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation of the naming process.},
	booktitle = {Proceedings of the 50th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Özbal, Gözde and Strapparava, Carlo},
	month = jul,
	year = {2012},
	pages = {703--711},
}

@inproceedings{miller_semeval-2017_2017,
	title = {{SemEval}-2017 {Task} 7: {Detection} and {Interpretation} of {English} {Puns}},
	isbn = {978-1-945626-55-5},
	doi = {10.18653/v1/S17-2005},
	abstract = {A pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetorical effect. Though a recurrent and expected feature in many discourse types, puns stymie traditional approaches to computational lexical semantics because they violate their one-sense-per-context assumption. This paper describes the first competitive evaluation for the automatic detection, location, and interpretation of puns. We describe the motivation for these tasks, the evaluation methods, and the manually annotated data set. Finally, we present an overview and discussion of the participating systems' methodologies, resources, and results.},
	booktitle = {Proceedings of the 11th {International} {Workshop} on {Semantic} {Evaluation}},
	author = {Miller, Tristan and Hempelmann, Christian F. and Gurevych, Iryna},
	month = aug,
	year = {2017},
	pages = {58--68},
}

@book{fellbaum_wordnet_1998,
	address = {Cambridge, MA},
	title = {{WordNet}: {An} {Electronic} {Lexical} {Database}},
	isbn = {978-0-262-06197-1},
	publisher = {MIT Press},
	editor = {Fellbaum, Christiane},
	year = {1998},
}

@incollection{hempelmann_puns_2017,
	address = {New York, NY},
	series = {Routledge {Handbooks} in {Linguistics}},
	title = {Puns: {Taxonomy} and {Phonology}},
	isbn = {978-1-138-84306-6},
	booktitle = {The {Routledge} {Handbook} of {Language} and {Humor}},
	publisher = {Routledge},
	author = {Hempelmann, Christian F. and Miller, Tristan},
	editor = {Attardo, Salvatore},
	month = feb,
	year = {2017},
	doi = {10.4324/9781315731162-8},
	pages = {95--108},
}

@article{aleksandrova_pun-based_2022,
	title = {Pun-based jokes and linguistic creativity: designing {3R}-module},
	volume = {10},
	doi = {10.7592/ejhr.2022.10.1.622},
	number = {1},
	journal = {The European Journal of Humour Research},
	author = {Aleksandrova, Elena Mikhailovna},
	month = apr,
	year = {2022},
	pages = {88--107},
}

@article{kembaren_challenges_2020,
	title = {{THE} {CHALLENGES} {AND} {SOLUTIONS} {OF} {TRANSLATING} {PUNS} {AND} {JOKES} {FROM} {ENGLISH} {TO} {INDONESIAN}},
	doi = {10.30829/vis.v16i2.807},
	journal = {VISION},
	author = {Kembaren, Farida Repelitawati Br},
	year = {2020},
}

@article{mizrabova_problems_2021,
	title = {{PROBLEMS} {OF} {CLASSIFICATION} {AND} {TRANSLATION} {OF} {PUN}},
	doi = {10.52297/2181-1466/2021/5/4/9},
	journal = {Scientific Reports of Bukhara State University},
	author = {Mizrabova, Jeren},
	year = {2021},
}

@inproceedings{thaler_varieties_2016,
	title = {Varieties of {Wordplay}},
	doi = {10.1515/9783110465600-003},
	author = {Thaler, Verena},
	year = {2016},
}

@inproceedings{thaler_varieties_2016-1,
	title = {Varieties of {Wordplay}},
	doi = {10.1515/9783110465600-003},
	author = {Thaler, Verena},
	year = {2016},
}

@misc{regattin_traduction_2021,
	title = {Traduction automatique et jeux de mots : l'incursion (ludique) d'un inculte},
	url = {https://motsmachines.github.io/2021/en/submissions/Mots-Machines-2021_paper_5.pdf},
	abstract = {Si la traduction/adaptation des jeux de mots n’est certainement pas, pour l’instant, à la portée de la traduction automatique (statistique ou neuronale, peu importe), cela est peut-être vrai pour d’assez nombreux traducteurs humains aussi. Dans les lignes qui suivent, nous invitons les lectrices et les lecteurs à participer à un test de Turing fait maison, en leur proposant les traductions de trois systèmes de TA en libre accès (DeepL, Google Traduction et Yandex) mélangées à celles de quelques traductrices et traducteurs littéraires ayant travaillé sur Alice’s Adventures in Wonderland.},
	author = {Regattin, Fabio},
	month = mar,
	year = {2021},
	note = {Place: Brest, Université de Bretagne occidentale},
}

@incollection{kolb_humancomputer_2022,
	title = {Human–{Computer} {Interaction} in {Pun} {Translation}},
	booktitle = {Using {Technologies} for {Creative}-{Text} {Translation}},
	publisher = {Routledge},
	author = {Kolb, Waltraud and Miller, Tristan},
	editor = {Hadley, James and Taivalkoski-Shilov, Kristiina and Teixeira, Carlos S. C. and Toral, Antonio},
	year = {2022},
	annote = {To appear},
}

@article{tuzzikriah_students_2021,
	title = {Students’ {Perception} on the {Problem} in {Translating} {Humor} {Text}},
	doi = {10.2991/assehr.k.210914.061},
	journal = {Proceedings of the Eighth International Conference on English Language and Teaching (ICOELT-8 2020)},
	author = {Tuzzikriah, Raihana and Ardi, Havid},
	year = {2021},
}

@article{rudenko_wordplay_2020,
	title = {{WORDPLAY} {AS} {A} {TRANSLATION} {PROBLEM}},
	doi = {10.18384/2310-712x-2020-2-78-85},
	journal = {Bulletin of the Moscow State Regional University (Linguistics)},
	author = {Rudenko, Elena S. and Bachieva, Rupija I.},
	year = {2020},
}

@article{bobchynets_lexico-semantic_2020,
	title = {{LEXICO}-{SEMANTIC} {MEANS} {OF} {CREATION} {OF} {PUN} {IN} {SPANISH} {AND} {PORTUGUESE} {JOKES}},
	doi = {10.26661/2414-1135-2020-80-1-11},
	journal = {Nova fìlologìâ},
	author = {Bobchynets, Lyubov},
	year = {2020},
}

@article{ardi_can_2022,
	title = {{CAN} {MACHINE} {TRANSLATIONS} {TRANSLATE} {HUMOROUS} {TEXTS}?},
	doi = {10.24036/humanus.v21i1.115698},
	journal = {Humanus},
	author = {Ardi, Havid and Hafizh, Muhd Al and Rezqi, Iftahur and Tuzzikriah, Raihana},
	year = {2022},
}

@inproceedings{miller_semeval-2017_2017-1,
	title = {{SemEval}-2017 {Task} 7: {Detection} and {Interpretation} of {English} {Puns}},
	isbn = {978-1-945626-55-5},
	doi = {10.18653/v1/S17-2005},
	booktitle = {Proceedings of the 11th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2017)},
	author = {Miller, Tristan and Hempelmann, Christian F. and Gurevych, Iryna},
	month = aug,
	year = {2017},
	pages = {58--68},
}

@inproceedings{miller_punsters_2019,
	title = {The {Punster}'s {Amanuensis}: {The} {Proper} {Place} of {Humans} and {Machines} in the {Translation} of {Wordplay}},
	doi = {10.26615/issn.2683-0078.2019_007},
	booktitle = {Proceedings of the {Second} {Workshop} on {Human}-{Informed} {Translation} and {Interpreting} {Technology} ({HiT}-{IT} 2019)},
	author = {Miller, Tristan},
	month = sep,
	year = {2019},
	note = {ISSN: 2683-0078},
	pages = {57--64},
}

@incollection{kolb_humancomputer_2022-1,
	title = {Human–{Computer} {Interaction} in {Pun} {Translation}},
	booktitle = {Using {Technologies} for {Creative}-{Text} {Translation}},
	publisher = {Routledge},
	author = {Kolb, Waltraud and Miller, Tristan},
	editor = {Hadley, James and Taivalkoski-Shilov, Kristiina and Teixeira, Carlos S. C. and Toral, Antonio},
	year = {2022},
	annote = {To appear},
}

@article{he_challenges_2021,
	title = {Challenges and {Countermeasures} of {Translation} {Teaching} in the {Era} of {Artificial} {Intelligence}},
	volume = {1881},
	doi = {10.1088/1742-6596/1881/2/022086},
	journal = {Journal of Physics: Conference Series},
	author = {He, Ying},
	year = {2021},
}

@article{laviosa_wordplay_2015,
	title = {Wordplay in advertising: {Form}, meaning and function},
	volume = {1},
	number = {1},
	journal = {Scripta Manent},
	author = {Laviosa, Sara},
	year = {2015},
	pages = {25--34},
}

@article{kovacs_translating_2020,
	title = {Translating {Humour} – {A} {Didactic} {Perspective}},
	volume = {12},
	journal = {Acta Universitatis Sapientiae, Philologica},
	author = {Kovács, Gabriella},
	year = {2020},
	pages = {68--83},
}

@article{hniedkova_peculiarities_2021,
	title = {Peculiarities of pun formation and translation of pun as a type of wordplay},
	volume = {2},
	doi = {10.32838/2710-4656/2021.1-2/44},
	number = {1},
	journal = {“Scientific notes of V. I. Vernadsky Taurida National University”, Series: “Philology. Journalism”},
	author = {Hniedkova, O G and Karpenko, Z O},
	year = {2021},
	note = {Publisher: V.I. Vernadsky Taurida National University},
	pages = {254--261},
}

@incollection{ermakova_clef_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CLEF} {Workshop} {JOKER}: {Automatic} {Wordplay} and {Humour} {Translation}},
	volume = {13186},
	isbn = {978-3-030-99738-0 978-3-030-99739-7},
	shorttitle = {{CLEF} {Workshop} {JOKER}},
	urldate = {2022-04-06},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Ermakova, Liana and Miller, Tristan and Puchalski, Orlane and Regattin, Fabio and Mathurin, Élise and Araújo, Sílvia and Bosser, Anne-Gwenn and Borg, Claudine and Bokiniec, Monika and Corre, Gaelle Le and Jeanjean, Benoît and Hannachi, Radia and Mallia, {\textbackslash}.Gorġ and Matas, Gordan and Saki, Mohamed},
	editor = {Hagen, Matthias and Verberne, Suzan and Macdonald, Craig and Seifert, Christin and Balog, Krisztian and Nørvåg, Kjetil and Setty, Vinay},
	year = {2022},
	doi = {10.1007/978-3-030-99739-7_45},
	pages = {355--363},
}

@article{regattin_traduire_2015,
	title = {Traduire les jeux de mots : une approche intégrée},
	url = {http://www.diacronia.ro/ro/indexing/details/A19521/pdf},
	number = {23},
	journal = {Atelier de traduction},
	author = {Regattin, Fabio},
	year = {2015},
	pages = {129--151},
}

@article{vidgen_directions_2020,
	title = {Directions in {Abusive} {Language} {Training} {Data}, a {Systematic} {Review}: {Garbage} {In}, {Garbage} out},
	volume = {15},
	doi = {10.1371/journal.pone.0243300},
	number = {12},
	journal = {PLOS One},
	author = {Vidgen, Bertie and Derczynski, Leon},
	year = {2020},
}

@article{cocco_gender_2012,
	title = {Gender {Stereotypes} and {Figurative} {Language} {Comprehension}},
	volume = {5},
	url = {http://www.humanamente.eu/index.php/HM/article/view/172},
	number = {22},
	journal = {Humana.Mente},
	author = {Cocco, Roberta and Ervas, Francesca},
	month = sep,
	year = {2012},
	pages = {43--56},
}

@incollection{strowel_artificial_2021,
	address = {Oxon, United Kingdom},
	title = {Artificial {Intelligence} and {Text} and {Data} {Mining}: a {Copyright} {Carol}},
	isbn = {978-1-00-315627-7},
	abstract = {Text and Data Mining (TDM) is a key tool for enabling artifcial intelligence (AI) innovation. As the BlueDot project has demonstrated, the possibility to mine works and data to identify patterns and correlations might be crucial for solving pressing societal problems (e.g., the prediction of the Covid-19 outbreak). The importance of TDM has been understood by the European legislator, which has introduced two specifcally tailored exceptions in the Copyright in the Digital Single Market Directive. This chapter argues that those new provisions still present several faws that risk to stife AI developments in Europe. The present chapter introduces an interpretative framework, based on the analysis of the infringement test, to rethink the rights of reproduction and extraction in line with the economic rationale of copyright and the database right. Furthermore, it makes suggestions to improve the TDM exceptions at national level. In conclusion, this chapter outlines the remaining challenges of private ordering and trade secrets for research and AI innovation.},
	booktitle = {The {Routledge} {Handbook} of {EU} {Copyright} {Law}},
	publisher = {Routledge},
	author = {Strowel, Alain and Ducato, Rossana},
	editor = {Rosati, Eleonora},
	year = {2021},
	doi = {10.4324/9781003156277-19},
	pages = {299--316},
}

@inproceedings{jain_equivoque_2019,
	title = {Equivoque: {Detection} and {Interpretation} of {English} {Puns}},
	doi = {10.1109/SMART46866.2019.9117433},
	booktitle = {Proceedigns of the 8th {International} {Conference} {System} {Modeling} and {Advancement} in {Research} {Trends}},
	author = {Jain, Aditi and Yadav, Pratishtha and Javed, Hira},
	year = {2019},
	pages = {262--265},
}

@incollection{miller_a255436_2015,
	title = {A255436: {Number} of {Distinct}, {Connected}, {Order}-n {Subgraphs} of the {Infinite} {Knight}'s {Graph}},
	abstract = {We present an integer sequence \$a(n)\$ corresponding to the number of distinct graphs of order \$n\$ where the vertices can be mapped to different squares of a chessboard such that the connected pairs of vertices are a knight's move apart.},
	booktitle = {The {On}-line {Encyclopedia} of {Integer} {Sequences}},
	author = {Miller, Tristan},
	month = feb,
	year = {2015},
}

@inproceedings{abualhaija_metaheuristic_2017,
	title = {Metaheuristic {Approaches} to {Lexical} {Substitution} and {Simplification}},
	volume = {1},
	isbn = {978-1-945626-34-0},
	abstract = {In this paper, we propose using metaheuristics—in particular, simulated annealing and the new D-Bees algorithm—to solve word sense disambiguation as an optimization problem within a knowledge-based lexical substitution system. We are the first to perform such an extrinsic evaluation of metaheuristics, for which we use two standard lexical substitution datasets, one English and one German. We find that D-Bees has robust performance for both languages, and performs better than simulated annealing, though both achieve good results. Moreover, the D-Bees–based lexical substitution system outperforms state-of-the-art systems on several evaluation metrics. We also show that D-Bees achieves competitive performance in lexical simplification, a variant of lexical substitution.},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Abualhaija, Sallam and Miller, Tristan and Eckle-Kohler, Judith and Gurevych, Iryna and Zimmermann, Karl-Heinz},
	month = apr,
	year = {2017},
	pages = {870--879},
}

@article{flom_impressions_2005,
	title = {Impressions from {Prac}{\textbackslash}{TeX}'05},
	volume = {26},
	issn = {0896-3207},
	number = {1},
	journal = {TUGboat: The Communications of the {\textbackslash}TeX Users Group},
	author = {Flom, Peter and Miller, Tristan},
	year = {2005},
	pages = {31--32},
}

@article{flom_impressions_2005-1,
	title = {Impressions from {Prac}{\textbackslash}{TeX}'05},
	volume = {2},
	issn = {1556-6994},
	number = {3},
	journal = {The Prac{\textbackslash}TeX Journal},
	author = {Flom, Peter and Miller, Tristan},
	month = jul,
	year = {2005},
}

@inproceedings{guggilla_cnn-_2016,
	title = {{CNN}- and {LSTM}-based {Claim} {Classification} in {Online} {User} {Comments}},
	isbn = {978-4-87974-702-0},
	abstract = {When processing arguments in online user interactive discourse, it is often necessary to determine their bases of support. In this paper, we describe a supervised approach, based on deep neural networks, for classifying the claims made in online arguments. We conduct experiments using convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) on two claim data sets compiled from online user comments. Using different types of distributional word embeddings, but without incorporating any rich, expensive set of features, we achieve a significant improvement over the state of the art for one data set (which categorizes arguments as factual vs.{\textbackslash} emotional), and performance comparable to the state of the art on the other data set (which categorizes claims according to their verifiability). Our approach has the advantages of using a generalized, simple, and effective methodology that works for claim categorization on different data sets and tasks.},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Computational} {Linguistics}: {Technical} {Papers}},
	author = {Guggilla, Chinnappa and Miller, Tristan and Gurevych, Iryna},
	month = dec,
	year = {2016},
	pages = {2740--2751},
}

@incollection{hempelmann_puns_2017-1,
	address = {New York, NY},
	series = {Routledge {Handbooks} in {Linguistics}},
	title = {Puns: {Taxonomy} and {Phonology}},
	isbn = {978-1-138-84306-6},
	booktitle = {The {Routledge} {Handbook} of {Language} and {Humor}},
	publisher = {Routledge},
	author = {Hempelmann, Christian F. and Miller, Tristan},
	editor = {Attardo, Salvatore},
	month = feb,
	year = {2017},
	doi = {10.4324/9781315731162-8},
	pages = {95--108},
}

@inproceedings{klein_security_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Security {Issues} for {Pervasive} {Personalized} {Communication} {Systems}},
	volume = {3450},
	isbn = {3-540-25521-4},
	abstract = {Technological progress allows us to equip any mobile phone with new functionalities, such as storing personalized information about its owner and using the corresponding personal profile for enabling communication to persons whose mobile phones represent similar profiles. However, this raises very specific security issues, in particular relating to the use of Bluetooth technology. Herein we consider such scenarios and related problems in privacy and security matters. We analyze in which respect certain design approaches may fail or succeed at solving these problems. We concentrate on methods for designing the user-related part of the communication service appropriately in order to enhance confidentiality.},
	booktitle = {Security in {Pervasive} {Computing}: {Proceedings} of the 2nd {International} {Conference} on {Security} in {Pervasive} {Computing}},
	publisher = {Springer},
	author = {Klein, Bertin and Miller, Tristan and Zilles, Sandra},
	editor = {Hutter, Dieter and Ullmann, Markus},
	month = apr,
	year = {2005},
	note = {ISSN: 0302-9743
event-place: Boppard, Germany},
	pages = {56--62},
}

@inproceedings{maher_efficient_2000,
	title = {Efficient {Defeasible} {Reasoning} {Systems}},
	isbn = {0-7695-0909-6},
	doi = {10.1109/TAI.2000.889898},
	abstract = {For many years, the non-monotonic reasoning community has focussed on highly expressive logics. Such logics have turned out to be computationally expensive, and have given little support to the practical use of non-monotonicreasoning. In this work we discuss defeasible logic, a less-expressive but more efficient non-monotonic logic. We report on two new implemented systems for defeasible logic: a query answering system employing a backward-chaining approach, and a forward-chaining implementation that computes all conclusions. Our experimental evaluation demonstrates that the systems can deal with large theories (up to hundreds of thousands of rules). We show that defeasible logic has linear complexity, which contrasts markedly with most other non-monotonic logics and helps to explain the impressive experimental results. We believe that defeasible logic, with its efficiency and simplicity, is a good candidate to be used as a modelling language for practical applications, including modelling of regulations and business rules.},
	booktitle = {Proceedings of the 12th {IEEE} {International} {Conference} on {Tools} with {Artificial} {Intelligence}},
	publisher = {IEEE Press},
	author = {Maher, Michael J. and Rock, Allan and Antoniou, Grigoris and Billington, David and Miller, Tristan},
	month = nov,
	year = {2000},
	note = {ISSN: 1082-3409},
	pages = {384--392},
}

@article{maher_efficient_2001,
	title = {Efficient {Defeasible} {Reasoning} {Systems}},
	volume = {10},
	issn = {0218-2130},
	doi = {10.1142/S0218213001000623},
	abstract = {For many years, the non-monotonic reasoning community has focussed on highly expressive logics. Such logics have turned out to be computationally expensive, and have given little support to the practical use of non-monotonicreasoning. In this work we discuss defeasible logic, a less-expressive but more efficient non-monotonic logic. We report on two new implemented systems for defeasible logic: a query answering system employing a backward-chaining approach, and a forward-chaining implementation that computes all conclusions. Our experimental evaluation demonstrates that the systems can deal with large theories (up to hundreds of thousands of rules). We show that defeasible logic has linear complexity, which contrasts markedly with most other non-monotonic logics and helps to explain the impressive experimental results. We believe that defeasible logic, with its efficiency and simplicity, is a good candidate to be used as a modelling language for practical applications, including modelling of regulations and business rules.},
	number = {4},
	journal = {International Journal on Artificial Intelligence Tools},
	author = {Maher, Michael J. and Rock, Allan and Antoniou, Grigoris and Billington, David and Miller, Tristan},
	month = dec,
	year = {2001},
	pages = {483--501},
}

@inproceedings{matuschek_language-independent_2014,
	title = {A {Language}-independent {Sense} {Clustering} {Approach} for {Enhanced} {WSD}},
	isbn = {978-3-934105-46-1},
	abstract = {We present a method for clustering word senses of a lexical-semantic resource by mapping them to those of another sense inventory. This is a promising way of reducing polysemy in sense inventories and consequently improving word sense disambiguation performance. In contrast to previous approaches, we use Dijkstra-WSA, a parameterizable alignment algorithm which is largely resource- and language-agnostic. To demonstrate this, we apply our technique to GermaNet, the German equivalent to WordNet. The GermaNet sense clusterings we induce through alignments to various collaboratively constructed resources achieve a significant boost in accuracy, even though our method is far less complex and less dependent on language-specific knowledge than past approaches.},
	booktitle = {Proceedings of the 12th {Konferenz} zur {Verarbeitung} natürlicher {Sprache}},
	publisher = {Universitätsverlag Hildesheim},
	author = {Matuschek, Michael and Miller, Tristan and Gurevych, Iryna},
	editor = {Ruppenhofer, Josef and Faaß, Gertrud},
	month = oct,
	year = {2014},
	pages = {11--21},
}

@techreport{miller_essay_2001,
	type = {Technical {Report}},
	title = {Essay {Assessment} with {Latent} {Semantic} {Analysis}},
	number = {CSRG-440},
	institution = {Department of Computer Science, University of Toronto},
	author = {Miller, Tristan},
	month = may,
	year = {2001},
}

@article{miller_essay_2003,
	title = {Essay {Assessment} with {Latent} {Semantic} {Analysis}},
	volume = {29},
	issn = {0735-6331},
	doi = {10.2190/W5AR-DYPW-40KX-FL99},
	abstract = {Latent semantic analysis (LSA) is an automated, statistical technique for comparing the semantic similarity of words or documents. In this paper, I examine the application of LSA to automated essay scoring. I compare LSA methods to earlier statistical methods for assessing essay quality, and critically review contemporary essay-scoring systems built on LSA, including the Intelligent Essay Assessor, Summary Street, State the Essence, Apex, and Select-a-Kibitzer. Finally, I discuss current avenues of research, including LSA's application to computer-measured readability assessment and to automatic summarization of student essays.},
	number = {4},
	journal = {Journal of Educational Computing Research},
	author = {Miller, Tristan},
	month = dec,
	year = {2003},
	pages = {495--512},
}

@phdthesis{miller_generating_2003,
	type = {M.{Sc}.{\textbackslash} thesis},
	title = {Generating {Coherent} {Extracts} of {Single} {Documents} {Using} {Latent} {Semantic} {Analysis}},
	abstract = {A major problem with automatically-produced summaries in general, and extracts in particular, is that the output text often lacks textual coherence. Our goal is to improve the textual coherence of automatically produced extracts. We developed and implemented an algorithm which builds an initial extract composed solely of topic sentences, and then recursively fills in the lacunae by providing linking material from the original text between semantically dissimilar sentences. Our summarizer differs in architecture from most others in that it measures semantic similarity with latent semantic analysis (LSA), a factor analysis technique based on the vector-space model of information retrieval. We believed that the deep semantic relations discovered by LSA would assist in the identification and correction of abrupt topic shifts in the summaries. However, our experiments did not show a statistically significant difference in the coherence of summaries produced by our system as compared with a non-LSA version.},
	school = {Department of Computer Science, University of Toronto},
	author = {Miller, Tristan},
	month = mar,
	year = {2003},
}

@inproceedings{miller_latent_2003,
	title = {Latent {Semantic} {Analysis} and the {Construction} of {Coherent} {Extracts}},
	isbn = {954-90906-6-3},
	abstract = {We describe a language-neutral automatic summarization system which aims to produce coherent extracts. It builds an initial extract composed solely of topic sentences, and then recursively fills in the topical lacunae by providing linking material between semantically dissimilar sentences. While experiments with human judges did not prove a statistically significant increase in textual coherence with the use of a latent semantic analysis module, we found a strong positive correlation between coherence and overall summary quality.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing}},
	author = {Miller, Tristan},
	editor = {Angelova, Galia and Bontcheva, Kalina and Mitkov, Ruslan and Nicolov, Nicolas and Nikolov, Nikolai},
	month = sep,
	year = {2003},
	pages = {270--277},
}

@incollection{miller_latent_2004,
	address = {Amsterdam/Philadelphia},
	series = {Current {Issues} in {Linguistic} {Theory}},
	title = {Latent {Semantic} {Analysis} and the {Construction} of {Coherent} {Extracts}},
	volume = {260},
	isbn = {1-58811-618-2},
	abstract = {We describe a language-neutral automatic summarization system which aims to produce coherent extracts. It builds an initial extract composed solely of topic sentences, and then recursively fills in the topical lacunae by providing linking material between semantically dissimilar sentences. While experiments with human judges did not prove a statistically significant increase in textual coherence with the use of a latent semantic analysis module, we found a strong positive correlation between coherence and overall summary quality.},
	booktitle = {Recent {Advances} in {Natural} {Language} {Processing} {III}},
	publisher = {John Benjamins},
	author = {Miller, Tristan},
	editor = {Nicolov, Nicolas and Botcheva, Kalina and Angelova, Galia and Mitkov, Ruslan},
	year = {2004},
	doi = {10.1075/cilt.260.31mil},
	note = {ISSN: 0304-0763},
	pages = {277--286},
}

@inproceedings{miller_attention-based_2005,
	address = {New York, NY},
	title = {Attention-based {Information} {Retrieval} {Using} {Eye} {Tracker} {Data}},
	isbn = {978-1-59593-163-4},
	doi = {10.1145/1088622.1088672},
	abstract = {We describe eFISK, an automated keyword extraction system which unobtrusively measures the user's attention in order to isolate and identify those areas of a written document the reader finds of greatest interest. Attention is measured by use of eye-tracking hardware consisting of a desk-mounted infrared camera which records various data about the user's eye. The keywords thus identified are subsequently used in the back end of an information retrieval system to help the user find other documents which contain information of interest to him. Unlike traditional IR techniques which compare documents simply on the basis of common terms withal, our system also accounts for the weights users implicitly attach to certain words or sections of the source document. We describe a task-based user study which compares the utility of standard relevance feedback techniques to the keywords and keyphrases discovered by our system in finding other relevant documents from a corpus.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Knowledge} {Capture}},
	publisher = {ACM},
	author = {Miller, Tristan and Agne, Stefan},
	editor = {Clark, Peter and Schreiber, Guus},
	month = sep,
	year = {2005},
	pages = {209--210},
}

@article{miller_biblet_2005,
	title = {Biblet: {A} {Portable} {\textbackslash}{BibTeX}{\textbackslash} {Bibliography} {Style} for {Generating} {Highly} {Customizable} {XHTML}},
	volume = {26},
	issn = {0896-3207},
	abstract = {We present Biblet, a set of BibTeX bibliography styles (bst) which generate XHTML from BibTeX databases. Unlike other BibTeX to XML{\textbackslash}slashHTML converters, Biblet is written entirely in the native BibTeX style language and therefore works “out of the box” on any system that runs BibTeX. Features include automatic conversion of LaTeX symbols to HTML or Unicode entities; customizable graphical hyperlinks to PostScript, PDF, DVI, LaTeX, and HTML resources; support for nonstandard but common fields such as day, isbn, and abstract; hideable text blocks; and output of the original BibTeX entry for sharing citations. Biblet's highly structured XHTML output means that bibliography appearance to can be drastically altered simply by specifying a Cascading Style Sheet (CSS), or easily postprocessed with third-party XML, HTML, or text processing tools. We compare and contrast Biblet to other common converters, describe basic usage of Biblet, give examples of how to produce custom-formatted bibliographies, and provide a basic overview of Biblet internals for those wishing to modify the style file itself.},
	number = {1},
	journal = {TUGboat: The Communications of the {\textbackslash}TeX Users Group},
	author = {Miller, Tristan},
	year = {2005},
	pages = {85--96},
}

@article{miller_producing_2005,
	title = {Producing {Beautiful} {Slides} with {\textbackslash}{LaTeX}: {An} {Introduction} to the {HA}-prosper {Package}},
	volume = {2},
	issn = {1556-6994},
	abstract = {In this paper, we present HA-prosper, a {\textbackslash}LaTeX package for creating overhead slides. We describe the features of the package and give examples of their use. We also discuss what advantages there are to producing slides with LaTeX versus the presentation software typically bundled with today's office suites.},
	number = {1},
	journal = {The Prac{\textbackslash}TeX Journal},
	author = {Miller, Tristan},
	month = apr,
	year = {2005},
}

@article{miller_using_2005,
	title = {Using the {RPM} {Package} {Manager} for {\textbackslash}{LaTeXTeX} {Packages}},
	volume = {26},
	issn = {0896-3207},
	abstract = {RPM is a package management system which provides a uniform, automated way for users to install, upgrade, and uninstall programs. Because RPM is the default software distribution format for many operating systems (particularly GNU/Linux), users may find it useful to manage their library of TeX-related packages using RPM. This article explains how to produce RPM files for TeX software, either for personal use or for public distribution. We also explain how a (La)TeX user can find, install, and remove TeX-related RPM packages.},
	number = {1},
	journal = {TUGboat: The Communications of the {\textbackslash}TeX Users Group},
	author = {Miller, Tristan},
	year = {2005},
	pages = {17--28},
}

@article{miller_creare_2006,
	title = {Creare splendide slade con {\textbackslash}{LaTeX}: {Un}'introduzione al pacchetto {HA}-prosper [{Producing} {Beautiful} {Slides} with {\textbackslash}{LaTeX}: {An} {Introduction} to the {HA}-prosper {Package}]},
	abstract = {In questo articolo verrà presentato HA-prosper, un pacchetto LaTeX per la creazione di sofisticate slide. Ne descriveremo le caratteristiche mostrandone alcuni esempi d'uso. Inoltre, discuteremo quali vantaggi si possono trarre dal tipo di approccio, proprio della filosofia LaTeX, in rapporto agli altri tipi di programmi per presentazioni che generalmente sono presenti nelle attuali suite di applicazioni per ufficio.},
	number = {47},
	journal = {Pluto Journal},
	author = {Miller, Tristan},
	month = may,
	year = {2006},
	annote = {Translated by Gabriele Zucchetta},
}

@inproceedings{miller_word_2006,
	title = {Word {Completion} with {Latent} {Semantic} {Analysis}},
	volume = {1},
	isbn = {978-0-7695-2521-1},
	doi = {10.1109/ICPR.2006.1191},
	abstract = {Current word completion tools rely mostly on statistical or syntactic knowledge. Can using semantic knowledge improve the completion task? We propose a language-independent word completion algorithm which uses latent semantic analysis (LSA) to model the semantic context of the word being typed. We find that a system using this algorithm alone achieves keystroke savings of 56\% and a hit rate of 42\%. This represents improvements of 4.3\% and 12\%, respectively, over existing approaches.},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Pattern} {Recognition}},
	publisher = {IEEE Press},
	author = {Miller, Tristan and Wolf, Elisabeth},
	editor = {Tang, Yuan Yan and Wang, S. Patrick and Lorette, G. and Yeung, Daniel So and Yan, Hong},
	month = aug,
	year = {2006},
	note = {ISSN: 1051-4651},
	pages = {1252--1255},
}

@inproceedings{miller_exploiting_2009,
	title = {Exploiting {Latent} {Semantic} {Relations} in {Highly} {Linked} {Hypertext} for {Information} {Retrieval} in {Wikis}},
	abstract = {Good hypertext writing style mandates that link texts clearly indicate the nature of the link target. While this guideline is routinely ignored in HTML, the lightweight markup languages used by wikis encourage or even force hypertext authors to use semantically appropriate link texts. This property of wiki hypertext makes it an ideal candidate for processing with latent semantic analysis, a factor analysis technique for finding latent transitive relations among natural-language documents. In this study, we design, implement, and test an LSA-based information retrieval system for wikis. Instead of a full-text index, our system indexes only link texts and document titles. Nevertheless, its precision exceeds that of a popular full-text search engine, and is comparable to that of PageRank-based systems such as Google.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing}},
	publisher = {ACM Press},
	author = {Miller, Tristan and Klein, Bertin and Wolf, Elisabeth},
	editor = {Angelova, Galia and Bontcheva, Kalina and Mitkov, Ruslan and Nicolov, Nicolas and Nikolov, Nikolai},
	month = sep,
	year = {2009},
	pages = {241--245},
}

@inproceedings{miller_using_2012,
	title = {Using {Distributional} {Similarity} for {Lexical} {Expansion} in {Knowledge}-based {Word} {Sense} {Disambiguation}},
	abstract = {We explore the contribution of distributional information for purely knowledge-based word sense disambiguation. Specifically, we use a distributional thesaurus, computed from a large parsed corpus, for lexical expansion of context and sense information.This bridges the lexical gap that is seen as the major obstacle for word overlap–based approaches.We apply this mechanism to two traditional knowledge-based methods and show that distributional information significantly improves disambiguation results across several data sets.This improvement exceeds the state of the art for disambiguation without sense frequency information—a situation which is especially encountered with new domains or languages for which no sense-annotated corpus is available.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Computational} {Linguistics}},
	author = {Miller, Tristan and Biemann, Chris and Zesch, Torsten and Gurevych, Iryna},
	editor = {Kay, Martin and Boitet, Christian},
	month = dec,
	year = {2012},
	pages = {1781--1796},
}

@inproceedings{miller_dkpro_2013,
	title = {{DKPro} {WSD}: {A} {Generalized} {UIMA}-based {Framework} for {Word} {Sense} {Disambiguation}},
	abstract = {Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.},
	booktitle = {Proceedings of the 51st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({System} {Demonstrations})},
	author = {Miller, Tristan and Erbs, Nicolai and Zorn, Hans-Peter and Zesch, Torsten and Gurevych, Iryna},
	month = aug,
	year = {2013},
	pages = {37--42},
}

@inproceedings{miller_wordnetwikipediawiktionary_2014,
	title = {{WordNet}–{Wikipedia}–{Wiktionary}: {Construction} of a {Three}-way {Alignment}},
	isbn = {978-2-9517408-8-4},
	abstract = {The coverage and quality of conceptual information contained in lexical semantic resources is crucial for many tasks in natural language processing. Automatic alignment of complementary resources is one way of improving this coverage and quality; however, past attempts have always been between pairs of specific resources. In this paper we establish some set-theoretic conventions for describing concepts and their alignments, and use them to describe a method for automatically constructing \$n\$-way alignments from arbitrary pairwise alignments. We apply this technique to the production of a three-way alignment from previously published WordNet–Wikipedia and WordNet–Wiktionary alignments. We then present a quantitative and informal qualitative analysis of the aligned resource. The three-way alignment was found to have greater coverage, an enriched sense representation, and coarser sense granularity than both the original resources and their pairwise alignments, though this came at the cost of accuracy. An evaluation of the induced word sense clusters in a word sense disambiguation task showed that they were no better than random clusters of equivalent granularity. However, use of the alignments to enrich a sense inventory with additional sense glosses did significantly improve the performance of a baseline knowledge-based WSD algorithm.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Language} {Resources} and {Evaluation}},
	publisher = {European Language Resources Association},
	author = {Miller, Tristan and Gurevych, Iryna},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asunción and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2014},
	pages = {2094--2100},
}

@inproceedings{miller_automatic_2015,
	title = {Automatic {Disambiguation} of {English} {Puns}},
	volume = {1},
	isbn = {978-1-941643-72-3},
	doi = {10.3115/v1/P15-1070},
	abstract = {Traditional approaches to word sense disambiguation (WSD) rest on the assumption that there exists a single, unambiguous communicative intention underlying every word in a document. However, writers sometimes intend for a word to be interpreted as simultaneously carrying multiple distinct meanings. This deliberate use of lexical ambiguity—{\textbackslash}emphi.e., punning—is a particularly common source of humour. In this paper we describe how traditional, language-agnostic WSD approaches can be adapted to “disambiguate” puns, or rather to identify their double meanings. We evaluate several such approaches on a manually sense-annotated corpus of English puns and observe performance exceeding that of some knowledge-based and supervised baselines.},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Miller, Tristan and Gurevych, Iryna},
	month = jul,
	year = {2015},
	pages = {719--729},
}

@inproceedings{miller_germeval_2015,
	title = {{GermEval} 2015: {LexSub} – {A} {Shared} {Task} for {German}-language {Lexical} {Substitution}},
	abstract = {Lexical substitution is a task in which participants are given a word in a short context and asked to provide a list of synonyms appropriate for that context. This paper describes GermEval 2015: LexSub, the first shared task for automated lexical substitution on German-language text. We describe the motivation for this task, the evaluation methods, and the manually annotated data set used to train and test the participating systems. Finally, we present an overview and discussion of the participating systems' methodologies, resources, and results.},
	booktitle = {Proceedings of {GermEval} 2015: {LexSub}},
	author = {Miller, Tristan and Benikova, Darina and Abualhaija, Sallam},
	month = sep,
	year = {2015},
	pages = {1--9},
}

@phdthesis{miller_adjusting_2016,
	type = {Dr.-{Ing}.{\textbackslash} thesis},
	title = {Adjusting {Sense} {Representations} for {Word} {Sense} {Disambiguation} and {Automatic} {Pun} {Interpretation}},
	school = {Department of Computer Science, Technische Universität Darmstadt},
	author = {Miller, Tristan},
	month = apr,
	year = {2016},
}

@inproceedings{miller_sense-annotating_2016,
	title = {Sense-annotating a {Lexical} {Substitution} {Data} {Set} with {Ubyline}},
	isbn = {978-2-9517408-9-1},
	abstract = {We describe the construction of GLASS, a newly sense-annotated version of the German lexical substitution data set used at the GermEval 2015: LexSub shared task. Using the two annotation layers, we conduct the first known empirical study of the relationship between manually applied word senses and lexical substitutions. We find that synonymy and hypernymy{\textbackslash}slashhyponymy are the only semantic relations directly linking targets to their substitutes, and that substitutes in the target's hypernymy{\textbackslash}slashhyponymy taxonomy closely align with the synonyms of a single GermaNet synset. Despite this, these substitutes account for a minority of those provided by the annotators. The results of our analysis accord with those of a previous study on English-language data (albeit with automatically induced word senses), leading us to suspect that the sense–substitution relations we discovered may be of a universal nature. We also tentatively conclude that relatively cheap lexical substitution annotations can be used as a knowledge source for automatic WSD. Also introduced in this paper is Ubyline, the web application used to produce the sense annotations. Ubyline presents an intuitive user interface optimized for annotating lexical sample data, and is readily adaptable to sense inventories other than GermaNet.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Language} {Resources} and {Evaluation}},
	publisher = {European Language Resources Association},
	author = {Miller, Tristan and Khemakhem, Mohamed and Eckart de Castilho, Richard and Gurevych, Iryna},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Grobelnik, Marko and Maegaard, Bente and Mariani, Joseph and Moreno, Asunción and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2016},
	pages = {828--835},
}

@article{miller_towards_2016,
	title = {Towards the {Automatic} {Detection} and {Identification} of {English} {Puns}},
	volume = {4},
	issn = {2307-700X},
	abstract = {Lexical polysemy, a fundamental characteristic of all human languages, has long been regarded as a major challenge to machine translation, human–computer interaction, and other applications of computational natural language processing (NLP). Traditional approaches to automatic word sense disambiguation (WSD) rest on the assumption that there exists a single, unambiguous communicative intention underlying every word in a document. However, writers sometimes intend for a word to be interpreted as simultaneously carrying multiple distinct meanings. This deliberate use of lexical ambiguity — {\textbackslash}emphi.e.,{\textbackslash} punning — is a particularly common source of humour, and therefore has important implications for how NLP systems process documents and interact with users. In this paper we make a case for research into computational methods for the detection of puns in running text and for the isolation of the intended meanings. We discuss the challenges involved in adapting principles and techniques from WSD to humorously ambiguous text, and outline our plans for evaluating WSD-inspired systems in a dedicated pun identification task. We describe the compilation of a large manually annotated corpus of puns and present an analysis of its properties. While our work is principally concerned with simple puns which are monolexemic and homographic ({\textbackslash}emphi.e.,{\textbackslash} exploiting single words which have different meanings but are spelled identically), we touch on the challenges involved in processing other types.},
	number = {1},
	journal = {European Journal of Humour Research},
	author = {Miller, Tristan and Turković, Mladen},
	month = jan,
	year = {2016},
	pages = {59--75},
}

@inproceedings{miller_ofaiukp_2019,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {{OFAI}–{UKP} at {HAHA}@{IberLEF2019}: {Predicting} the {Humorousness} of {Tweets} {Using} {Gaussian} {Process} {Preference} {Learning}},
	volume = {2421},
	abstract = {Most humour processing systems to date make at best discrete, coarse-grained distinctions between the comical and the conventional, yet such notions are better conceptualized as a broad spectrum. In this paper, we present a probabilistic approach, a variant of Gaussian process preference learning (GPPL), that learns to rank and rate the humorousness of short texts by exploiting human preference judgments and automatically sourced linguistic annotations. We apply our system, which had previously shown good performance on English-language one-liners annotated with pairwise humorousness annotations, to the Spanish-language data set of the HAHA@IberLEF2019 evaluation campaign. We report system performance for the campaign's two subtasks, humour detection and funniness score prediction, and discuss some issues arising from the conversion between the numeric scores used in the HAHA@IberLEF2019 data and the pairwise judgment annotations required for our method.},
	booktitle = {Proceedings of the {Iberian} {Languages} {Evaluation} {Forum}},
	author = {Miller, Tristan and Do Dinh, Erik-Lân and Simpson, Edwin and Gurevych, Iryna},
	editor = {García Cumbreras, Miguel Ángel and Gonzalo, Julio and Martínez Cámara, Eugenio and Martínez Unanue, Raquel and Rosso, Paolo and Carrillo-de-Albornoz, Jorge and Montalvo, Soto and Chiruzzo, Luis and Collovini, Sandra and Guitiérrez, Yoan and Jiménez Zafra, Salud and Krallinger, Martin and Montes-y-Gómez, Manuel and Ortega-Bueno, Reynier and Rosá, Aiala},
	month = aug,
	year = {2019},
	note = {ISSN: 1613-0073},
	pages = {180--190},
}

@inproceedings{miller_punsters_2019-1,
	title = {The {Punster}'s {Amanuensis}: {The} {Proper} {Place} of {Humans} and {Machines} in the {Translation} of {Wordplay}},
	doi = {10.26615/issn.2683-0078.2019_007},
	abstract = {The translation of wordplay is one of the most extensively researched problems in translation studies, but it has attracted little attention in the fields of natural language processing and machine translation. This is because today's language technologies treat anomalies and ambiguities in the input as things that must be resolved in favour of a single “correct” interpretation, rather than preserved and interpreted in their own right. But if computers cannot yet process such creative language on their own, can they at least provide specialized support to translation professionals? In this paper, I survey the state of the art relevant to computational processing of humorous wordplay and put forth a vision of how existing theories, resources, and technologies could be adapted and extended to support interactive, computer-assisted translation.},
	booktitle = {Proceedings of the {Second} {Workshop} on {Human}-{Informed} {Translation} and {Interpreting} {Technology}},
	author = {Miller, Tristan},
	month = sep,
	year = {2019},
	note = {ISSN: 2683-0078},
	pages = {57--64},
}

@article{miller_reinhold_2019,
	title = {Reinhold {Aman} (1936–2019)},
	volume = {30.4729},
	journal = {The LINGUIST List},
	author = {Miller, Tristan},
	month = dec,
	year = {2019},
}

@inproceedings{miller_streamlined_2019,
	title = {A {Streamlined} {Method} for {Sourcing} {Discourse}-level {Argumentation} {Annotations} from the {Crowd}},
	volume = {1},
	isbn = {978-1-950737-13-0},
	doi = {10.18653/v1/N19-1177},
	abstract = {The study of argumentation and the development of argument mining tools depends on the availability of annotated data, which is challenging to obtain in sufficient quantity and quality. We present a method that breaks down a popular but relatively complex discourse-level argument annotation scheme into a simpler, iterative procedure that can be applied even by untrained annotators. We apply this method in a crowdsourcing setup and report on the reliability of the annotations obtained. The source code for a tool implementing our annotation method, as well as the sample data we obtained (4909 gold-standard annotations across 982 documents), are freely released to the research community. These are intended to serve the needs of qualitative research into argumentation, as well as of data-driven approaches to argument mining.},
	booktitle = {Proceedings of the 17th {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Miller, Tristan and Sukhareva, Maria and Gurevych, Iryna},
	month = jun,
	year = {2019},
	pages = {1790--1796},
}

@article{miller_dmitri_2020,
	title = {Dmitri {Borgmann}'s {Rotas} {Square} {Articles}},
	volume = {67},
	issn = {0029-3970},
	doi = {10.1093/notesj/gjaa113},
	abstract = {In 1979 and 1980, Word Ways: The Journal of Recreational Linguistics printed a series of articles on the early history, religious symbolism, and cultural significance of the rotas square, an ancient Latin-language palindromic word square. The articles were attributed to Dmitri A. Borgmann, the noted American writer on wordplay and former editor of Word Ways. While they attracted little attention at the time, some 35 years after their publication (and 29 years after Borgmann's death), questions began to be raised about their authorship. There is much internal and external evidence that, taken together, compellingly supports the notion that Borgmann did not write the articles himself. This paper surveys this evidence and solicits help in identifying the articles' original source.},
	number = {3},
	journal = {Notes and Queries},
	author = {Miller, Tristan},
	month = sep,
	year = {2020},
	pages = {431--432},
}

@article{miller_gpp_2020,
	title = {{GPP}, the {Generic} {Preprocessor}},
	volume = {5},
	issn = {2475-9066},
	doi = {10.21105/joss.02400},
	abstract = {In computer science, a preprocessor (or macro processor) is a tool that programatically alters its input, typically on the basis of inline annotations, to produce data that serves as input for another program. Preprocessors are used in software development and document processing workflows to translate or extend programming or markup languages, as well as for conditional or pattern-based generation of source code and text. Early preprocessors were relatively simple string replacement tools that were tied to specific programming languages and application domains, and while these have since given rise to more powerful, general-purpose tools, these often require the user to learn and use complex macro languages with their own syntactic conventions. In this paper, we present GPP, an extensible, general-purpose preprocessor whose principal advantage is that its syntax and behaviour can be customized to suit any given preprocessing task. This makes GPP of particular benefit to research applications, where it can be easily adapted for use with novel markup, programming, and control languages.},
	number = {51},
	journal = {Journal of Open Source Software},
	author = {Miller, Tristan and Auroux, Denis},
	month = jul,
	year = {2020},
}

@article{miller_predicting_2020,
	title = {Predicting the {Humorousness} of {Tweets} {Using} {Gaussian} {Process} {Preference} {Learning}},
	volume = {64},
	issn = {1135-5948},
	doi = {10.26342/2020-64-4},
	abstract = {Most humour processing systems to date make at best discrete, coarse-grained distinctions between the comical and the conventional, yet such notions are better conceptualized as a broad spectrum. In this paper, we present a probabilistic approach, a variant of Gaussian process preference learning (GPPL), that learns to rank and rate the humorousness of short texts by exploiting human preference judgments and automatically sourced linguistic annotations. We apply our system, which is similar to one that had previously shown good performance on English-language one-liners annotated with pairwise humorousness annotations, to the Spanish-language data set of the HAHA@IberLEF2019 evaluation campaign. We report system performance for the campaign's two subtasks, humour detection and funniness score prediction, and discuss some issues arising from the conversion between the numeric scores used in the HAHA@IberLEF2019 data and the pairwise judgment annotations required for our method.},
	journal = {Procesamiento del Lenguaje Natural},
	author = {Miller, Tristan and Do Dinh, Erik-Lân and Simpson, Edwin and Gurevych, Iryna},
	month = mar,
	year = {2020},
	pages = {37--44},
}

@article{miller_reinhold_2020,
	title = {Reinhold {Aman}, 1936–2019},
	volume = {32},
	issn = {0933-1719},
	doi = {10.1515/humor-2019-0085},
	number = {1},
	journal = {Humor: International Journal of Humor Research},
	author = {Miller, Tristan},
	month = feb,
	year = {2020},
	pages = {1--5},
}

@inproceedings{simpson_predicting_2019,
	title = {Predicting {Humorousness} and {Metaphor} {Novelty} with {Gaussian} {Process} {Preference} {Learning}},
	isbn = {978-1-950737-48-2},
	doi = {10.18653/v1/P19-1572},
	abstract = {The inability to quantify key aspects of creative language is a frequent obstacle to natural language understanding. To address this, we introduce novel tasks for evaluating the creativeness of language—namely, scoring and ranking text by humorousness and metaphor novelty. To sidestep the difficulty of assigning discrete labels or numeric scores, we learn from pairwise comparisons between texts. We introduce a Bayesian approach for predicting humorousness and metaphor novelty using Gaussian process preference learning (GPPL), which achieves a Spearman's ρ of 0.56 against gold using word embeddings and linguistic features. Our experiments show that given sparse, crowdsourced annotation data, ranking using GPPL outperforms best–worst scaling. We release a new dataset for evaluating humor containing 28,210 pairwise comparisons of 4,030 texts, and make our software freely available.},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Simpson, Edwin and Do Dinh, Erik-Lân and Miller, Tristan and Gurevych, Iryna},
	month = jul,
	year = {2019},
	pages = {5716--5728},
}

@inproceedings{stab_argumentext_2018,
	title = {{ArgumenText}: {Searching} for {Arguments} in {Heterogeneous} {Sources}},
	isbn = {978-1-948087-28-5},
	doi = {10.18653/v1/N18-5005},
	abstract = {Argument mining is a core technology for enabling argument search in large corpora. However, most current approaches fall short when applied to heterogeneous texts. In this paper, we present an argument retrieval system capable of retrieving sentential arguments for any given controversial topic. By analyzing the highest-ranked results extracted from Web sources, we found that our system covers 89\% of arguments found in expert-curated lists of arguments from an online debate portal, and also identifies additional valid arguments.},
	booktitle = {Proceedings of the 16th {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Demonstrations}},
	author = {Stab, Christian and Daxenberger, Johannes and Stahlhut, Chris and Miller, Tristan and Schiller, Benjamin and Tauchmann, Christopher and Eger, Steffen and Gurevych, Iryna},
	month = jun,
	year = {2018},
	pages = {21--25},
}

@inproceedings{stab_cross-topic_2018,
	title = {Cross-topic {Argument} {Mining} from {Heterogeneous} {Sources}},
	isbn = {978-1-948087-84-1},
	doi = {10.18653/v1/D18-1402},
	abstract = {Argument mining is a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches are designed for use only with specific text types and fall short when applied to heterogeneous texts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. We source annotations for over 25,000 instances covering eight controversial topics. We show that integrating topic information into bidirectional long short-term memory networks outperforms vanilla BiLSTMs by more than 3 percentage points in F₁ in two- and three-label cross-topic settings. We also show that these results can be further improved by leveraging additional data for topic relevance using multi-task learning.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Stab, Christian and Miller, Tristan and Schiller, Benjamin and Rai, Pranav and Gurevych, Iryna},
	month = oct,
	year = {2018},
	pages = {3664--3674},
}

@article{stab_cross-topic_2018-1,
	title = {Cross-topic {Argument} {Mining} from {Heterogeneous} {Sources} {Using} {Attention}-based {Neural} {Networks}},
	volume = {1802.05758},
	abstract = {Argument mining is a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches to argument mining are designed for use only with specific text types and fall short when applied to heterogeneous texts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. We source annotations for over 25,000 instances covering eight controversial topics. The results of cross-topic experiments show that our attention-based neural network generalizes best to unseen topics and outperforms vanilla BiLSTM models by 6\% in accuracy and 11\% in F-score.},
	journal = {ArXiv e-prints},
	author = {Stab, Christian and Miller, Tristan and Gurevych, Iryna},
	month = feb,
	year = {2018},
}

@inproceedings{uma_semeval-2021_2021,
	title = {{SemEval}-2021 {Task} 12: {Learning} with {Disagreements}},
	isbn = {978-1-954085-70-1},
	doi = {10.18653/v1/2021.semeval-1.41},
	abstract = {Disagreement between coders is ubiquitous in virtually all datasets annotated with human judgements in both natural language processing and computer vision. However, most supervised machine learning methods assume that a single preferred interpretation exists for each item, which is at best an idealization. The aim of the SemEval-2021 shared task on Learning with Disagreements (Le-wi-Di) was to provide a unified testing framework for methods for learning from data containing multiple and possibly contradictory annotations covering the best-known datasets containing information about disagreements for interpreting language and classifying images. In this paper we describe the shared task and its results.},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation}},
	author = {Uma, Alexandra and Fornaciari, Tommaso and Dumitrache, Anca and Miller, Tristan and Chamberlain, Jon and Plank, Barbara and Simpson, Edwin and Poesio, Massimo},
	month = aug,
	year = {2021},
	pages = {338--347},
}

@inproceedings{wockener_end--end_2021,
	title = {End-to-end {Style}-{Conditioned} {Poetry} {Generation}: {What} {Does} {It} {Take} to {Learn} from {Examples} {Alone}?},
	abstract = {In this work, we design an end-to-end model for poetry generation based on conditioned recurrent neural network (RNN) language models whose goal is to learn stylistic features (poem length, sentiment, alliteration, and rhyming) from examples alone. We show this model successfully learns the `meaning' of length and sentiment, as we can control it to generate longer or shorter as well as more positive or more negative poems. However, the model does not grasp sound phenomena like alliteration and rhyming, but instead exploits low-level statistical cues. Possible reasons include the size of the training data, the relatively low frequency and difficulty of these sublexical phenomena as well as model biases. We show that more recent GPT-2 models also have problems learning sublexical phenomena such as rhyming from examples alone.},
	booktitle = {Proceedings of the 5th {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics} for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature}},
	author = {Wöckener, Jörg and Haider, Thomas and Miller, Tristan and Nguyen, The-Khang and Nguyen, Thanh Tung Linh and Pham, Minh Vu and Belouadi, Jonas and Eger, Steffen},
	month = nov,
	year = {2021},
	pages = {57--66},
}

@inproceedings{wolf_use_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Use} of {Topic} {Models} for {Word} {Completion}},
	volume = {4139},
	isbn = {978-3-540-37334-6},
	doi = {10.1007/11816508_50},
	abstract = {We investigate the use of topic models, such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA), for word completion tasks. The advantage of using these models for such an application is twofold. On the one hand, they allow us to exploit semantic or contextual information when predicting candidate words for completion. On the other hand, these probabilistic models have been found to outperform classical latent semantic analysis (LSA) for modeling text documents. We describe a word completion algorithm that takes into account the semantic context of the word being typed. We also present evaluation metrics to compare different models being used in our study. Our experiments validate our hypothesis of using probabilistic models for semantic analysis of text documents and their application in word completion tasks.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Springer},
	author = {Wolf, Elisabeth and Vembu, Shankar and Miller, Tristan},
	editor = {Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo and Pahikkala, Tapio},
	month = aug,
	year = {2006},
	note = {ISSN: 0302-9743},
	pages = {500--511},
}

@article{xiang_well-behaved_1999,
	title = {A {Well}-behaved {Algorithm} for {Simulating} {Dependence} {Structures} of {Bayesian} {Networks}},
	volume = {1},
	issn = {1311-1728},
	abstract = {Automatic generation of Bayesian network (BN) structures (directed acyclic graphs) is an important step in experimental study of algorithms for inference in BNs and algorithms for learning BNs from data. Previously known simulation algorithms do not guarantee connectedness of generated structures or even successful genearation according to a user specification. We propose a simple, efficient and well-behaved algorithm for automatic generation of BN structures. The performance of the algorithm is demonstrated experimentally.},
	number = {8},
	journal = {International Journal of Applied Mathematics},
	author = {Xiang, Yang and Miller, Tristan},
	year = {1999},
	pages = {923--932},
}

@article{cohn_can_2022,
	title = {Can {We} {Diagram} the {Understanding} of {Visual} {Humour}?},
	volume = {12},
	issn = {2192-5283},
	abstract = {Purely visual humour, such as static cartoons, can be understood without language. That is, a suitably arranged scene of simple objects, with no accompanying text, is often enough to make us laugh—evidence that thinking (mental activity) happens before language. This raises the question of non-linguistic diagrammatic representation of visual humour, along with the mechanism of neural computation. In particular, we raise following questions: (1) How can we diagrammatically formalise visual humour? (2) How can these diagrammatic formalisms be processed by neural networks? (3) How can this neural computation deliver high-level schema that are similar to the script-opposition semantic theory of humour?},
	journal = {Dagstuhl Reports},
	author = {Cohn, Anthony G. and Dong, Tiansi and Hempelmann, Christian F. and Miller, Tristan and Mohsen, Siba and Rayz, Julia Taylor},
	year = {2022},
	annote = {To appear},
}

@inproceedings{miller_analysis_2014,
	address = {Komárno, Slovakia},
	title = {An {Analysis} of {Ambiguity} in {English} {Puns}},
	abstract = {Punning is a common source of verbal humour in which a word is used to evoke two or more distinct meanings simultaneously. The present work describes and analyzes a large corpus of English homographic puns manually annotated with senses from WordNet. We discuss the challenges in developing and applying the annotation scheme, introduce our annotation support tools, and present an analysis of selected morphological, syntactic, and semantic properties of the annotated examples. Particular focus is placed on the implications for computational approaches to detection of puns and identification of their opposing meanings.},
	booktitle = {International {Humour} {Symposium} [of the 4th {Hungarian} {Interdisciplinary} {Humour} {Conference}]: {Programme} and {Abstracts}},
	publisher = {J. Selye University, Faculty of Education, Department of Modern Philology},
	author = {Miller, Tristan},
	month = nov,
	year = {2014},
}

@inproceedings{miller_dont_2020,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Don't {Shun} the {Pun}: {On} the {Requirements} and {Constraints} for {Preserving} {Ambiguity} in the ({Machine}) {Translation} of {Humour}},
	volume = {2584},
	abstract = {How do we know when a translation is good? This seemingly simple question has long dogged human practitioners of translation, and has arguably taken on even greater importance in today’s world of fully automatic, end-to-end machine translation systems. Much of the difficulty in assessing translation quality is that different translations of the same text may be made for different purposes, each of which entails a unique set of requirements and constraints. This difficulty is compounded by ambiguities in the source text, which must be identified and then preserved or eliminated according to the needs of the translation and the (apparent) intent of the source text. In this talk, I survey the state of the art in linguistics, computational linguistics, translation, and machine translation as it relates to the notion of linguistic ambiguity in general, and intentional humorous ambiguity in particular. I describe the various constraints and requirements of different types of translations and provide examples of how various automatic and interactive techniques from natural language processing can be used to detect and then resolve or preserve linguistic ambiguities according to these constraints and requirements. In the vein of the “Translator’s Amanuensis” proposed by Martin Kay, I outline some specific proposals concerning how the hitherto disparate work in the aforementioned fields can be connected with a view to producing “machine-in-the-loop” computer-assisted translation (CAT) tools to assist human translators in selecting and implementing pun translation strategies in furtherance of the translation requirements. Throughout the talk, I will attempt to draw links with how this research relates to the requirements engineering community.},
	booktitle = {Joint {Proceedings} of {REFSQ}-2020 {Workshops}, {Doctoral} {Symposium}, {Live} {Studies} {Track}, and {Poster} {Track} co-located with the 26th {International} {Conference} on {Requirements} {Engineering}: {Foundation} for {Software} {Quality}},
	author = {Miller, Tristan},
	editor = {Sabetzadeh, Mehrdad and Vogelsang, Andreas and Abualhaija, Sallam and Borg, Markus and Dalpiaz, Fabiano and Daneva, Maya and Fernández, Nelly C. and Franch, Xavier and Fucci, Davide and Gervasi, Vincenzo and Groen, Eduard and Guizzardi, Renata and Herrmann, Andrea and Horkoff, Jennifer and Mich, Luisa and Perini, Anna and Susi, Angelo},
	month = mar,
	year = {2020},
	note = {ISSN: 1613-0073},
}

@inproceedings{preciado_assessing_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Assessing {Wordplay}-{Pun} classification from {JOKER} dataset with pretrained {BERT} humorous models},
	abstract = {Humor is one of the most subjective matters of human behavior since it includes a wide range of variables: sentiments, wordplay, double meanings structurally or phonetic, all of this within the construction of written humor. It is important to assess the humor from a different point of view since this variability tends to provide insight into the true structure or the main core of the humoristic dilemma, as we know the range of humor is so diverse that it presents a high skilled problem even on the simplest tasks. Pre-trained base Bert and DistilBert models trained with a humorous one-liners dataset were used, these trained models were tested with a merged dataset from JOKER from data of tasks 1 and task 3, the collected data was trimmed from duplicated records and special characters to create a final dataset with 3,601 humorous sentences. Under this experiment we try to see if our models were able to detect a different humor from the initial type with which they were trained, it was noted that both methods are able to successfully classify another type of humor. On the one hand, it was expected that the pre-trained models would be able to classify at least a portion of the humor in the data set, the results obtained were much better than anticipated, obtaining 95.64\% for BERT and 92.58\% for DistilBERT, the models were really able to identify humor, an analysis of the worst and best cases were taken into account.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Preciado, Victor Manuel Palma and Sidorov, Grigori and Preciado, Carolina Palma},
	year = {2022},
	pages = {6},
}

@inproceedings{talec-bernard_how_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {How good can an automatic translation of {Pokémon} names be?},
	abstract = {For those of you who are not familiar with the successful franchise of Pokémon, it revolves around imaginary creatures often representing animals mixed with objects, plants, etc... Their names reflect these characteristics and are, most of the time, wordplays.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Talec-Bernard, Léa},
	year = {2022},
	pages = {2},
}

@inproceedings{noauthor_translating_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Translating, transcribing, transmitting and transcending a pun : why playing with words is far from being punless/pointless},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	year = {2022},
	pages = {4},
}

@inproceedings{glemarec_use_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Use of {SimpleT5} for the {CLEF} workshop {JokeR}: {Automatic} {Pun} and {Humor} {Translation}},
	abstract = {In this paper, we present our work in the JokeR workshop. JokeR is a project aiming to study the strategies of localization of humor and puns and to create a multilingual parallel corpus and evaluation metrics in order to put a step forward to Automatic Pun and Humor Translation. 3 Tasks were proposed and to predict the requested fields for each of them, we trained the T5 model specialist in natural language processing tasks. Then we applied the models on test datasets, generating the runs as requested. By doing this we propose a simple method to study the translatability of puns. Also, we propose another way to implement our method to obtain more varied results. All this is to study the multilingual character of wordplay.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Glemarec, Loc},
	year = {2022},
	pages = {11},
}

@inproceedings{glemarec_generating_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Generating {Humourous} {Puns} in {French}},
	abstract = {Recent work have tackled the problem of generating puns in english, based on the corpus of english puns from Sem-Eval 2017 Task 7. In this paper, we report on experiments on generating French puns based on the data released for the CLEF 2022 JOKER and inspired by methods for generating English puns.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Glémarec, Loic and Bosser, Anne-Gwenn and Ermakova, Liana},
	year = {2022},
	pages = {8},
}

@inproceedings{paul_history_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {A history of {Classification} and {JokeR}’s {Reach}},
	abstract = {This essay will serve as a base for people who wish to learn about JokeR’s classification reach. As a base, it will start with a short resume of wordplay classification and by defining the way we apprehend terms that have not yet made consensus.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Paul, Campen and Albin, Digue},
	year = {2022},
	pages = {8},
}

@inproceedings{epimakhova_using_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Using machine learning to classify and interpret wordplay},
	abstract = {In this work, we study the first task “Classify and explain instances of wordplay” set as part of the workshop project “Joker”. Pilot Task 1 includes both classification and interpretation components. We use the most common methods to convert text into features. This study is based on the ML technics for elaborating an automated process of classifying and predicting missing features for test data. We use the bag-of-words model and the statistical measure of word frequency - inverse document frequency to convert text to features. Also, we apply polynomial naive Bayesian classifier and Logistic Regression to classify and predict text (with and without preprocessing). The result of the work is tables of accuracy for English and French wordplays. Examples of mostly unsuccessful and isolated relatively successful interpretations are presented. Prediction accuracy for isolated cases is less than 1\%. Accuracy for the manipulation type is also not high, about 50-60\%. Accuracy for other features is quite high, above 93\%.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Epimakhova, Aygyul},
	year = {2022},
	pages = {6},
}

@inproceedings{tiedemann_opus-mt_2020,
	title = {{OPUS}-{MT} – {Building} open translation services for the {World}},
	booktitle = {Proceedings of the 22nd {Annual} {Conference} of the {European} {Association} for {Machine} {Translation}},
	publisher = {European Association for Machine Translation},
	author = {Tiedemann, Jörg and Thottingal, Santhosh},
	year = {2020},
}

@inproceedings{galeano_ljgg_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {{LJGG} @ {CLEF} {JOKER} {Task} 3: {An} improved solution joining with dataset from task},
	abstract = {In this paper, we describe the results of our participation to the CLEF JOKER 2022 Task 3, "Translate entire phrases containing wordplay". The purpose of this task is the translation of English phrases to French, that contain a wordplay.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Galeano, Leopoldo Jesús Gutiérrez},
	year = {2022},
	pages = {7},
}

@inproceedings{boccou_using_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Using a ’punning scheme’ as a template to translating puns},
	abstract = {This paper details a method I used for translating puns from English into French. This method nicknamed ‘punning scheme’ stands as a framework to support creativity, accuracy and consistency when translating puns.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Boccou, Julien},
	year = {2022},
	pages = {13},
}

@inproceedings{albin_automatic_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Automatic translation of wordplay},
	abstract = {This essay tackles the subject of automatic humour translation, and the differences linked to the different translation engines used. The used data mainly comprises homographs, paronyms and portmanteau words. A large amount of data and homogeneity in its evaluation were necessary to criticise the AI’s ability to creatively translate.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Albin, Digue and Paul, Campen},
	year = {2022},
	pages = {10},
}

@inproceedings{pascu_modeling_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Modeling a software of semantic text analysis},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Pascu, Anca Christine},
	year = {2022},
	pages = {10},
}

@inproceedings{regattin_automatic_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Automatic {Translation} and {Wordplay}: {An} {Amateur}’s ({Playful}) {Thoughts}},
	abstract = {While translating/adapting wordplay is certainly not, for the time being, within the reach of machine translation (be it statistical or neural), the same may hold true for quite a few human translators as well. In my contribution, I will invite readers to take part in a sort of homemade Turing test, by asking them to uncover the translations of three open access MT systems (DeepL, Google Translate, and Yandex), mixed with the renderings of the same passages made by some human translators. All our corpus will consist of English to French translations of some excerpts taken from Lewis Carroll’s Alice’s Adventures in Wonderland.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Regattin, Fabio},
	year = {2022},
	pages = {7},
}

@inproceedings{delarche_translation-oriented_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {A translation-oriented categorisation of wordplays},
	abstract = {We show here that it is possible to pragmatically base a wordplay categorisation scheme on the different tools and algorithms needed to support the (more or less) automated detection and translation of wordplays, especially those whose translation by existing generic tools is impossible or inadequate. This approach also provides a way to develop metrics for quantifying the quality of wordplay translations.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Delarche, Michel},
	year = {2022},
	pages = {6},
}

@inproceedings{zhao_integrating_2018,
	address = {Brussels, Belgium},
	title = {Integrating {Transformer} and {Paraphrase} {Rules} for {Sentence} {Simplification}},
	url = {https://www.aclweb.org/anthology/D18-1355},
	abstract = {Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from machine translation studies and implicitly learned simplification mapping rules from normal-simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we propose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state-of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at https://github.com/Sanqiang/text\_simplification.},
	urldate = {2020-11-20},
	booktitle = {Proc. of {EMNLP} 2018},
	publisher = {ACL},
	author = {Zhao, Sanqiang and Meng, Rui and He, Daqing and Saptono, Andi and Parmanto, Bambang},
	month = oct,
	year = {2018},
	pages = {3164--3173},
}

@inproceedings{he_pun_2019,
	address = {Minneapolis, Minnesota},
	title = {Pun {Generation} with {Surprise}},
	url = {https://aclanthology.org/N19-1172},
	doi = {10.18653/v1/N19-1172},
	abstract = {We tackle the problem of generating a pun sentence given a pair of homophones (e.g., “died” and “dyed”). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a large corpus. In this paper, we propose an unsupervised approach to pun generation based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., “dyed”) and the distant context, but a strong association between the alternative word (e.g., “died”) and the immediate context. We instantiate the surprisal principle in two ways: (i) as a measure based on the ratio of probabilities given by a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30\% of the time, doubling the success rate of a neural generation baseline.},
	urldate = {2021-10-19},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {He, He and Peng, Nanyun and Liang, Percy},
	month = jun,
	year = {2019},
	pages = {1734--1744},
}

@inproceedings{dhanani_fastmt_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {{FAST}‗{MT} {PARTICIPATION} {FOR} {THE} {JOKER} {CLEF}-2022 {AUTOMATIC} {PUN} {AND} {HUMAN} {TRANSLATION} {TASKS}},
	abstract = {This paper presents the solution proposed by team FAST Machine Translation to the shared tasks of JOKER CLEF 2022 Automatic pun and human translation. State-of-the-art Transformer-based models are used to solve the three tasks introduced in the JOKER CLEF workshop. The Transformer model is a kind of neural network that tries to learn the contextual information from the sequential data by implicitly comprehending the existing relationships. In task 1, given a piece of text, we need to classify/explain any instance of wordplay is present in it or not. The proposed solution to task 1 combines the pipeline of token classification, text classification, and text generation. In task 2, we need to translate single words (nouns) containing a wordplay. This task is mapped to the problem of question answering (Q/A) on programmatically extracted texts from the OPUS parallel corpus. In task 3, contestants are required to translate the entire phrase containing the wordplay. Sequence-to sequence translation models are used to solve this task. The team has adopted different strategies for each task as they suited to the requirements therein. The paper reports proposed solutions, implementation details, experimental studies, and results obtained in JOKER CLEF 2022 automatic pun and human translation tasks.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Dhanani, Farhan and Rafi, Muhammad and Tahir, Muhammad Atif},
	year = {2022},
	pages = {14},
}

@inproceedings{tiedemann_parallel_2012,
	address = {Istanbul, Turkey},
	title = {Parallel {Data}, {Tools} and {Interfaces} in {OPUS}},
	url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf},
	abstract = {This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'12)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Tiedemann, Jörg},
	month = may,
	year = {2012},
	pages = {2214--2218},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/20-074.html},
	number = {140},
	journal = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
	pages = {1--67},
}

@inproceedings{samofalova_greenpeace_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Greenpeace {Memes} for {Communicating} {Climate} {Change}},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Samofalova, Yuliya and Catellani, Andrea and Cougnon, Louise-Amélie},
	year = {2022},
	pages = {3},
}

@inproceedings{damoy_translation_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Translation strategies: adaptation and equivalence 1—{Joker} contest},
	abstract = {This paper deals with wordplay translation strategies based on the Joker project and contest. Humour and wordplay are discussed a lot in linguistics as well as in translation studies. Both very difficult to render, they have been the subjects of debates, theses and analysis in many languages. The goal is to analyze the data collected through the Joker contest and find examples of conventional puns or translations of cultural allusions to explain the strategies relevant when going from one language to another. Moreover, my personal experience with the Joker contest and the Joker classification is used as an example of the reasoning taken when translating a wordplay.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Damoy, Aurianne},
	year = {2022},
	pages = {6},
}

@misc{senel_can_2022,
	title = {Can ({A}){I} or can't ({A}){I}? {Translation} of wordplays},
	shorttitle = {Can ({A}){I} or can't ({A}){I}?},
	url = {https://uazhlt-ms-program.github.io/ling-582-course-blog/senel/shared-task},
	abstract = {In which I attempt at one of the most daunting tasks in NLP: wordplays},
	language = {en},
	urldate = {2022-06-29},
	author = {Senel, Burak},
	year = {2022},
	file = {Snapshot:C\:\\Users\\liana\\Zotero\\storage\\YM3ZJULI\\shared-task.html:text/html},
}

@inproceedings{arroubat_clef_2022,
	address = {Bologna, Italy},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {{CLEF} {Workshop}: {Automatic} {Pun} and {Humour} {Translation} {Task}},
	abstract = {Today, all professional translators use machine translation as a basis for understanding the general outline of a text. One of the challenges of translation today is to translate humor, which is often considered untranslatable or difficult to translate. Indeed, the transfer of the humorous side, requires a thorough knowledge of the target language and culture. Humor is often done in a language using on puns that are known by the translation difficulty that arises in the structural ambiguity. Currently, there is still a lot to be done for humor translation, just as there is little research today on machine translation of humor. In the present work, I am interested in the automatic translation of humor from French to English and vice versa, using artificial intelligence techniques. First, I am interested in the data provided for this purpose, I have tried to clean them up. The second task consists in classifying the word sets by providing semantic interpretations.},
	language = {en},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022 – {Conference} and {Labs} of the {Evaluation} {Forum}, {Bologna}, {Italy}, {September} 5th to 8th, 2022},
	publisher = {CEUR-WS.org},
	author = {Arroubat, Hakima},
	year = {2022},
}

@book{faggioli_proceedings_2022,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Proceedings of the {Working} {Notes} of {CLEF} 2022: {Conference} and {Labs} of the {Evaluation} {Forum}},
	editor = {Faggioli, Guglielmo and Ferro, Nicola and Hanbury, Allan and Potthast, Martin},
	year = {2022},
}

@inproceedings{faggioli_overview_2022,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Overview of the {CLEF} 2022 {JOKER} {Task} 1: {Classify} and {Explain} {Instances} of {Wordplay}},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022: {Conference} and {Labs} of the {Evaluation} {Forum}},
	author = {Ermakova, Liana and Regattin, Fabio and Miller, Tristan and Bosser, Anne-Gwenn and Araújo, Sílvia and Borg, Claudine and Corre, Gaelle Le and Boccou, Julien and Digue, Albin and Damoy, Aurianne and Campen, Paul and Puchalski, Orlane},
	editor = {Faggioli, Guglielmo and Ferro, Nicola and Hanbury, Allan and Potthast, Martin},
	year = {2022},
}

@inproceedings{faggioli_overview_2022-1,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Overview of the {CLEF} 2022 {JOKER} {Task} 3: {Pun} {Translation} from {English} into {French}},
	booktitle = {Proceedings of the {Working} {Notes} of {CLEF} 2022: {Conference} and {Labs} of the {Evaluation} {Forum}},
	author = {Ermakova, Liana and Regattin, Fabio and Miller, Tristan and Bosser, Anne-Gwenn and Borg, Claudine and Jeanjean, Benoît and Mathurin, Elise and Corre, Gaelle Le and Hannachi, Radia and Araújo, Sílvia and Boccou, Julien and Digue, Albin and Damoy, Aurianne},
	editor = {Faggioli, Guglielmo and Ferro, Nicola and Hanbury, Allan and Potthast, Martin},
	year = {2022},
}
